{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Latex Project (ZIP file) using `claude-v1.3-100k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import openai\n",
    "import time\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import gradio as gr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper - Get Arxiv Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "def process_tex_files_from_zip(zip_file_path):\n",
    "    # Open the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, mode=\"r\") as zip:\n",
    "        # Load all .tex files into memory, including their subdirectories\n",
    "        tex_files = {\n",
    "            member.filename: zip.read(member.filename).decode(\"utf-8\")\n",
    "            for member in zip.infolist()\n",
    "            if member.filename.endswith(\".tex\")\n",
    "        }\n",
    "\n",
    "    # Pattern to match \\input{filename} and \\include{filename}\n",
    "    pattern = re.compile(r\"\\\\(input|include){(.*?)}\")\n",
    "\n",
    "    # Function to replace \\input{filename} and \\include{filename} with file contents\n",
    "    def replace_includes(text):\n",
    "        output = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                command, filename = match.groups()\n",
    "                # LaTeX automatically adds .tex extension for \\input and \\include commands\n",
    "                if not filename.endswith(\".tex\"):\n",
    "                    filename += \".tex\"\n",
    "                if filename in tex_files:\n",
    "                    output.append(replace_includes(tex_files[filename]))\n",
    "                else:\n",
    "                    output.append(f\"% {line} % FILE NOT FOUND\")\n",
    "            else:\n",
    "                output.append(line)\n",
    "        return \"\\n\".join(output)\n",
    "\n",
    "    if \"main.tex\" in tex_files:\n",
    "        # Start with the contents of main.tex\n",
    "        main_tex = replace_includes(tex_files[\"main.tex\"])\n",
    "    else:\n",
    "        # No main.tex, concatenate all .tex files\n",
    "        main_tex = \"\\n\".join(replace_includes(text) for text in tex_files.values())\n",
    "\n",
    "    return main_tex\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Client(api_key=os.environ['ANTHROPIC_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualQA:\n",
    "    def __init__(self, client, model=\"claude-v1.3-100k\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.context = \"\"\n",
    "        self.questions = []\n",
    "        self.responses = []\n",
    "\n",
    "    def load_text(self, text):\n",
    "        self.context = text\n",
    "\n",
    "    def ask_question(self, question):\n",
    "        leading_prompt = \"Consider the text document below:\"\n",
    "        trailing_prompt = (\n",
    "            \"Now answer the following question, use Markdown to format your answer.\"\n",
    "        )\n",
    "        prompt = f\"{anthropic.HUMAN_PROMPT} {leading_prompt}\\n\\n{self.context}\\n\\n{trailing_prompt}\\n\\n{anthropic.HUMAN_PROMPT} {question} {anthropic.AI_PROMPT}\"\n",
    "        response = self.client.completion_stream(\n",
    "            prompt=prompt,\n",
    "            stop_sequences=[anthropic.HUMAN_PROMPT],\n",
    "            max_tokens_to_sample=6000,\n",
    "            model=self.model,\n",
    "            stream=False,\n",
    "        )\n",
    "        responses = [data for data in response]\n",
    "        self.questions.append(question)\n",
    "        self.responses.append(responses)\n",
    "        return responses\n",
    "\n",
    "    def clear_context(self):\n",
    "        self.context = \"\"\n",
    "        self.questions = []\n",
    "        self.responses = []\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        del state[\"client\"]\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "        self.client = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_to_pdf(question, answer, output_filename):\n",
    "    latex_template = r\"\"\"\n",
    "    \\documentclass{{standalone}}\n",
    "    \\usepackage[utf8]{{inputenc}}\n",
    "    \\usepackage{{amsmath}}\n",
    "    \\usepackage{{amssymb}}\n",
    "    \\usepackage{{hyperref}}\n",
    "    \\usepackage{{varwidth}}\n",
    "    \\usepackage{{adjustbox}}\n",
    "    \\begin{{document}}\n",
    "    \\begin{{adjustbox}}{{margin=5mm}}\n",
    "    \\begin{{varwidth}}{{\\linewidth}}\n",
    "    \\textbf{{Question:}} \\\\\n",
    "    {question} \\\\\n",
    "    \\textbf{{Answer:}} \\\\\n",
    "    {answer}\n",
    "    \\end{{varwidth}}\n",
    "    \\end{{adjustbox}}\n",
    "    \\end{{document}}\n",
    "    \"\"\"\n",
    "\n",
    "    answer = answer[0][\"completion\"]\n",
    "    latex_content = latex_template.format(question=question, answer=answer)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        tex_file = Path(temp_dir) / \"qa.tex\"\n",
    "        pdf_file = Path(temp_dir) / \"qa.pdf\"\n",
    "\n",
    "        with open(tex_file, \"w\") as f:\n",
    "            f.write(latex_content)\n",
    "\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"pdflatex\",\n",
    "                \"-interaction=nonstopmode\",\n",
    "                \"-output-directory\",\n",
    "                temp_dir,\n",
    "                tex_file,\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "\n",
    "        shutil.copy(pdf_file, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59099"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_source = process_tex_files_from_zip(\"/Users/taesiri/Downloads/[MR] - SceneDetectorGPT.zip\")\n",
    "len(latex_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%% bare_jrnl.tex\n",
      "%% V1.4b\n",
      "%% 2015/08/26\n",
      "%% by Michael Shell\n",
      "%% see http://www.michaelshell.org/\n",
      "%% for current contact information.\n",
      "%%\n",
      "%% This is a skeleton file demonstrating the use of IEEEtran.cls\n",
      "%% (requires IEEEtran.cls version 1.8b or later) with an IEEE\n",
      "%% journal paper.\n",
      "%%\n",
      "%% Support sites:\n",
      "%% http://www.michaelshell.org/tex/ieeetran/\n",
      "%% http://www.ctan.org/pkg/ieeetran\n",
      "%% and\n",
      "%% http://www.ieee.org/\n",
      "\n",
      "%%*************************************************************************\n",
      "%% Legal Notice:\n",
      "%% This code is offered as-is without any warranty either expressed or\n",
      "%% implied; without even the implied warranty of MERCHANTABILITY or\n",
      "%% FITNESS FOR A PARTICULAR PURPOSE! \n",
      "%% User assumes all risk.\n",
      "%% In no event shall the IEEE or any contributor to this code be liable for\n",
      "%% any damages or losses, including, but not limited to, incidental,\n",
      "%% consequential, or any other damages, resulting from the use or misuse\n",
      "%% of any information contained here.\n",
      "%%\n",
      "%% All comments are the opinions of their respective authors and are not\n",
      "%% necessarily endorsed by the IEEE.\n",
      "%%\n",
      "%% This work is distributed under the LaTeX Project Public License (LPPL)\n",
      "%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,\n",
      "%% distributed and modified. A copy of the LPPL, version 1.3, is included\n",
      "%% in the base LaTeX documentation of all distributions of LaTeX released\n",
      "%% 2003/12/01 or later.\n",
      "%% Retain all contribution notices and credits.\n",
      "%% ** Modified files should be clearly indicated as such, including  **\n",
      "%% ** renaming them and changing author support contact information. **\n",
      "%%*************************************************************************\n",
      "\n",
      "\n",
      "% *** Authors should verify (and, if needed, correct) their LaTeX system  ***\n",
      "% *** with the testflow diagnostic prior to trusting their LaTeX platform ***\n",
      "% *** with production work. The IEEE's font choices and paper sizes can   ***\n",
      "% *** trigger bugs that do not appear when using other class files.       ***                          ***\n",
      "% The testflow support page is at:\n",
      "% http://www.michaelshell.org/tex/testflow/\n",
      "\n",
      "\n",
      "\n",
      "\\documentclass[10pt,journal,compsoc]{IEEEtran}\n",
      "%\n",
      "% If IEEEtran.cls has not been installed into the LaTeX system files,\n",
      "% manually specify the path to it like:\n",
      "% \\documentclass[journal]{../sty/IEEEtran}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% Some very useful LaTeX packages include:\n",
      "% (uncomment the ones you want to load)\n",
      "\n",
      "\n",
      "% *** MISC UTILITY PACKAGES ***\n",
      "%\n",
      "%\\usepackage{ifpdf}\n",
      "% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional\n",
      "% compilation based on whether the output is pdf or dvi.\n",
      "% usage:\n",
      "% \\ifpdf\n",
      "%   % pdf code\n",
      "% \\else\n",
      "%   % dvi code\n",
      "% \\fi\n",
      "% The latest version of ifpdf.sty can be obtained from:\n",
      "% http://www.ctan.org/pkg/ifpdf\n",
      "% Also, note that IEEEtran.cls V1.7 and later provides a builtin\n",
      "% \\ifCLASSINFOpdf conditional that works the same way.\n",
      "% When switching from latex to pdflatex and vice-versa, the compiler may\n",
      "% have to be run twice to clear warning/error messages.\n",
      "\n",
      "\n",
      "\n",
      "% *** CITATION PACKAGES ***\n",
      "%\n",
      "%\\usepackage{cite}\n",
      "% cite.sty was written by Donald Arseneau\n",
      "% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package\n",
      "% \\cite{} output to follow that of the IEEE. Loading the cite package will\n",
      "% result in citation numbers being automatically sorted and properly\n",
      "% \"compressed/ranged\". e.g., [1], [9], [2], [7], [5], [6] without using\n",
      "% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's\n",
      "% \\cite will automatically add leading space, if needed. Use cite.sty's\n",
      "% noadjust option (cite.sty V3.8 and later) if you want to turn this off\n",
      "% such as if a citation ever needs to be enclosed in parenthesis.\n",
      "% cite.sty is already installed on most LaTeX systems. Be sure and use\n",
      "% version 5.0 (2009-03-20) and later if using hyperref.sty.\n",
      "% The latest version can be obtained at:\n",
      "% http://www.ctan.org/pkg/cite\n",
      "% The documentation is contained in the cite.sty file itself.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% *** GRAPHICS RELATED PACKAGES ***\n",
      "%\n",
      "\\ifCLASSINFOpdf\n",
      "  % \\usepackage[pdftex]{graphicx}\n",
      "  % declare the path(s) where your graphic files are\n",
      "  % \\graphicspath{{../pdf/}{../jpeg/}}\n",
      "  % and their extensions so you won't have to specify these with\n",
      "  % every instance of \\includegraphics\n",
      "  % \\DeclareGraphicsExtensions{.pdf,.jpeg,.png}\n",
      "\\else\n",
      "  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx\n",
      "  % will default to the driver specified in the system graphics.cfg if no\n",
      "  % driver is specified.\n",
      "  % \\usepackage[dvips]{graphicx}\n",
      "  % declare the path(s) where your graphic files are\n",
      "  % \\graphicspath{{../eps/}}\n",
      "  % and their extensions so you won't have to specify these with\n",
      "  % every instance of \\includegraphics\n",
      "  % \\DeclareGraphicsExtensions{.eps}\n",
      "\\fi\n",
      "% graphicx was written by David Carlisle and Sebastian Rahtz. It is\n",
      "% required if you want graphics, photos, etc. graphicx.sty is already\n",
      "% installed on most LaTeX systems. The latest version and documentation\n",
      "% can be obtained at: \n",
      "% http://www.ctan.org/pkg/graphicx\n",
      "% Another good source of documentation is \"Using Imported Graphics in\n",
      "% LaTeX2e\" by Keith Reckdahl which can be found at:\n",
      "% http://www.ctan.org/pkg/epslatex\n",
      "%\n",
      "% latex, and pdflatex in dvi mode, support graphics in encapsulated\n",
      "% postscript (.eps) format. pdflatex in pdf mode supports graphics\n",
      "% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure\n",
      "% that all non-photo figures use a vector format (.eps, .pdf, .mps) and\n",
      "% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats\n",
      "% which can result in \"jaggedy\"/blurry rendering of lines and letters as\n",
      "% well as large increases in file sizes.\n",
      "%\n",
      "% You can find documentation about the pdfTeX application at:\n",
      "% http://www.tug.org/applications/pdftex\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% *** MATH PACKAGES ***\n",
      "%\n",
      "%\\usepackage{amsmath}\n",
      "% A popular package from the American Mathematical Society that provides\n",
      "% many useful and powerful commands for dealing with mathematics.\n",
      "%\n",
      "% Note that the amsmath package sets \\interdisplaylinepenalty to 10000\n",
      "% thus preventing page breaks from occurring within multiline equations. Use:\n",
      "%\\interdisplaylinepenalty=2500\n",
      "% after loading amsmath to restore such page breaks as IEEEtran.cls normally\n",
      "% does. amsmath.sty is already installed on most LaTeX systems. The latest\n",
      "% version and documentation can be obtained at:\n",
      "% http://www.ctan.org/pkg/amsmath\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% *** SPECIALIZED LIST PACKAGES ***\n",
      "%\n",
      "%\\usepackage{algorithmic}\n",
      "% algorithmic.sty was written by Peter Williams and Rogerio Brito.\n",
      "% This package provides an algorithmic environment fo describing algorithms.\n",
      "% You can use the algorithmic environment in-text or within a figure\n",
      "% environment to provide for a floating algorithm. Do NOT use the algorithm\n",
      "% floating environment provided by algorithm.sty (by the same authors) or\n",
      "% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated\n",
      "% algorithm float types and packages that provide these will not provide\n",
      "% correct IEEE style captions. The latest version and documentation of\n",
      "% algorithmic.sty can be obtained at:\n",
      "% http://www.ctan.org/pkg/algorithms\n",
      "% Also of interest may be the (relatively newer and more customizable)\n",
      "% algorithmicx.sty package by Szasz Janos:\n",
      "% http://www.ctan.org/pkg/algorithmicx\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% *** ALIGNMENT PACKAGES ***\n",
      "%\n",
      "%\\usepackage{array}\n",
      "% Frank Mittelbach's and David Carlisle's array.sty patches and improves\n",
      "% the standard LaTeX2e array and tabular environments to provide better\n",
      "% appearance and additional user controls. As the default LaTeX2e table\n",
      "% generation code is lacking to the point of almost being broken with\n",
      "% respect to the quality of the end results, all users are strongly\n",
      "% advised to use an enhanced (at the very least that provided by array.sty)\n",
      "% set of table tools. array.sty is already installed on most systems. The\n",
      "% latest version and documentation can be obtained at:\n",
      "% http://www.ctan.org/pkg/array\n",
      "\n",
      "\n",
      "% IEEEtran contains the IEEEeqnarray family of commands that can be used to\n",
      "% generate multiline equations as well as matrices, tables, etc., of high\n",
      "% quality.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% *** SUBFIGURE PACKAGES ***\n",
      "%\\ifCLASSOPTIONcompsoc\n",
      "%  \\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}\n",
      "%\\else\n",
      "%  \\usepackage[caption=false,font=footnotesize]{subfig}\n",
      "%\\fi\n",
      "% subfig.sty, written by Steven Douglas Cochran, is the modern replacement\n",
      "% for subfigure.sty, the latter of which is no longer maintained and is\n",
      "% incompatible with some LaTeX packages including fixltx2e. However,\n",
      "% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty\n",
      "% which will override IEEEtran.cls' handling of captions and this will result\n",
      "% in non-IEEE style figure/table captions. To prevent this problem, be sure\n",
      "% and invoke subfig.sty's \"caption=false\" package option (available since\n",
      "% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls\n",
      "% handling of captions.\n",
      "% Note that the Computer Society format requires a larger sans serif font\n",
      "% than the serif footnote size font used in traditional IEEE formatting\n",
      "% and thus the need to invoke different subfig.sty package options depending\n",
      "% on whether compsoc mode has been enabled.\n",
      "%\n",
      "% The latest version and documentation of subfig.sty can be obtained at:\n",
      "% http://www.ctan.org/pkg/subfig\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% *** FLOAT PACKAGES ***\n",
      "%\n",
      "%\\usepackage{fixltx2e}\n",
      "% fixltx2e, the successor to the earlier fix2col.sty, was written by\n",
      "% Frank Mittelbach and David Carlisle. This package corrects a few problems\n",
      "% in the LaTeX2e kernel, the most notable of which is that in current\n",
      "% LaTeX2e releases, the ordering of single and double column floats is not\n",
      "% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a\n",
      "% single column figure to be placed prior to an earlier double column\n",
      "% figure.\n",
      "% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's\n",
      "% corrections already built into the system in which case a warning will\n",
      "% be issued if an attempt is made to load fixltx2e.sty as it is no longer\n",
      "% needed.\n",
      "% The latest version and documentation can be found at:\n",
      "% http://www.ctan.org/pkg/fixltx2e\n",
      "\n",
      "\n",
      "%\\usepackage{stfloats}\n",
      "% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e\n",
      "% the ability to do double column floats at the bottom of the page as well\n",
      "% as the top. (e.g., \"\\begin{figure*}[!b]\" is not normally possible in\n",
      "% LaTeX2e). It also provides a command:\n",
      "%\\fnbelowfloat\n",
      "% to enable the placement of footnotes below bottom floats (the standard\n",
      "% LaTeX2e kernel puts them above bottom floats). This is an invasive package\n",
      "% which rewrites many portions of the LaTeX2e float routines. It may not work\n",
      "% with other packages that modify the LaTeX2e float routines. The latest\n",
      "% version and documentation can be obtained at:\n",
      "% http://www.ctan.org/pkg/stfloats\n",
      "% Do not use the stfloats baselinefloat ability as the IEEE does not allow\n",
      "% \\baselineskip to stretch. Authors submitting work to the IEEE should note\n",
      "% that the IEEE rarely uses double column equations and that authors should try\n",
      "% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty\n",
      "% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in\n",
      "% such ways.\n",
      "% Do not attempt to use stfloats with fixltx2e as they are incompatible.\n",
      "% Instead, use Morten Hogholm'a dblfloatfix which combines the features\n",
      "% of both fixltx2e and stfloats:\n",
      "%\n",
      "% \\usepackage{dblfloatfix}\n",
      "% The latest version can be found at:\n",
      "% http://www.ctan.org/pkg/dblfloatfix\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "%\\ifCLASSOPTIONcaptionsoff\n",
      "%  \\usepackage[nomarkers]{endfloat}\n",
      "% \\let\\MYoriglatexcaption\\caption\n",
      "% \\renewcommand{\\caption}[2][\\relax]{\\MYoriglatexcaption[#2]{#2}}\n",
      "%\\fi\n",
      "% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and \n",
      "% Axel Sommerfeldt. This package may be useful when used in conjunction with \n",
      "% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that\n",
      "% submissions have lists of figures/tables at the end of the paper and that\n",
      "% figures/tables without any captions are placed on a page by themselves at\n",
      "% the end of the document. If needed, the draftcls IEEEtran class option or\n",
      "% \\CLASSINPUTbaselinestretch interface can be used to increase the line\n",
      "% spacing as well. Be sure and use the nomarkers option of endfloat to\n",
      "% prevent endfloat from \"marking\" where the figures would have been placed\n",
      "% in the text. The two hack lines of code above are a slight modification of\n",
      "% that suggested by in the endfloat docs (section 8.4.1) to ensure that\n",
      "% the full captions always appear in the list of figures/tables - even if\n",
      "% the user used the short optional argument of \\caption[]{}.\n",
      "% IEEE papers do not typically make use of \\caption[]'s optional argument,\n",
      "% so this should not be an issue. A similar trick can be used to disable\n",
      "% captions of packages such as subfig.sty that lack options to turn off\n",
      "% the subcaptions:\n",
      "% For subfig.sty:\n",
      "% \\let\\MYorigsubfloat\\subfloat\n",
      "% \\renewcommand{\\subfloat}[2][\\relax]{\\MYorigsubfloat[]{#2}}\n",
      "% However, the above trick will not work if both optional arguments of\n",
      "% the \\subfloat command are used. Furthermore, there needs to be a\n",
      "% description of each subfigure *somewhere* and endfloat does not add\n",
      "% subfigure captions to its list of figures. Thus, the best approach is to\n",
      "% avoid the use of subfigure captions (many IEEE journals avoid them anyway)\n",
      "% and instead reference/explain all the subfigures within the main caption.\n",
      "% The latest version of endfloat.sty and its documentation can obtained at:\n",
      "% http://www.ctan.org/pkg/endfloat\n",
      "%\n",
      "% The IEEEtran \\ifCLASSOPTIONcaptionsoff conditional can also be used\n",
      "% later in the document, say, to conditionally put the References on a \n",
      "% page by themselves.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% *** PDF, URL AND HYPERLINK PACKAGES ***\n",
      "%\n",
      "%\\usepackage{url}\n",
      "% url.sty was written by Donald Arseneau. It provides better support for\n",
      "% handling and breaking URLs. url.sty is already installed on most LaTeX\n",
      "% systems. The latest version and documentation can be obtained at:\n",
      "% http://www.ctan.org/pkg/url\n",
      "% Basically, \\url{my_url_here}.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% *** Do not adjust lengths that control margins, column widths, etc. ***\n",
      "% *** Do not use packages that alter fonts (such as pslatex).         ***\n",
      "% There should be no need to do such things with IEEEtran.cls V1.6 and later.\n",
      "% (Unless specifically asked to do so by the journal or conference you plan\n",
      "% to submit to, of course. )\n",
      "\n",
      "\n",
      "\\usepackage{adjustbox}\n",
      "\n",
      "\\usepackage{subcaption}\n",
      "\\usepackage{multirow}\n",
      "\\usepackage{xspace}\n",
      "\\usepackage{listings}\n",
      "\\usepackage{booktabs}\n",
      "\\usepackage{array}\n",
      "\\RequirePackage{fontawesome}\n",
      "\\usepackage{xcolor, soul}\n",
      "\\usepackage{hyperref}\n",
      "\\usepackage{xcolor, soul}\n",
      "\\sethlcolor{yellow}\n",
      "\\usepackage{graphicx}\n",
      "% \\usepackage{subfig}\n",
      "\\usepackage{tikz}\n",
      "\\usepackage{listings}\n",
      "\\usepackage{booktabs}\n",
      "\\usepackage[T1]{fontenc}\n",
      "\\newcommand\\todo[1]{\\textcolor{red}{#1}}\n",
      "\\newcommand{\\reddit}[2]{\n",
      "    \\href{https://www.reddit.com/r/GamePhysics/comments/#1}{{\\small\\faExternalLink}\\xspace#2}%\n",
      "}\n",
      "\n",
      "\\newcommand{\\sceneGPT}[0]{SceneDetectiveGPT\\xspace}\n",
      "\n",
      "\n",
      "% correct bad hyphenation here\n",
      "\\hyphenation{op-tical net-works semi-conduc-tor}\n",
      "\n",
      "\n",
      "\\begin{document}\n",
      "%\n",
      "% paper title\n",
      "% Titles are generally capitalized except for words such as a, an, and, as,\n",
      "% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually\n",
      "% not capitalized unless they are the first or last word of the title.\n",
      "% Linebreaks \\\\ can be used within to get better formatting as desired.\n",
      "% Do not put math or special symbols in the title.\n",
      "\\title{\\sceneGPT: Efficient Test Oracle Generation for Video Game Cutscenes}\n",
      "%\n",
      "%\n",
      "% author names and IEEE memberships\n",
      "% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break\n",
      "% a structure at a ~ so this keeps an author's name from being broken across\n",
      "% two lines.\n",
      "% use \\thanks{} to gain access to the first footnote area\n",
      "% a separate \\thanks must be used for each paragraph as LaTeX2e's \\thanks\n",
      "% was not built to handle multiple paragraphs\n",
      "%\n",
      "\n",
      "\\author{Mohammad Reza Taesiri,\n",
      "        and~Cor-Paul Bezemer% <-this % stops a space\n",
      "\\thanks{Mohammad Reza and Cor-Paul are with the Analytics of Software, GAmes and Repository Data (ASGAARD) lab, University of Alberta, Edmonton, Canada. E-mail: (see \\todo{https://github.com/asgaardlab/detective}). \\protect\\\\\n",
      "}% <-this % stops a space\n",
      "}\n",
      "\n",
      "\n",
      "% note the % following the last \\IEEEmembership and also \\thanks - \n",
      "% these prevent an unwanted space from occurring between the last author name\n",
      "% and the end of the author line. i.e., if you had this:\n",
      "% \n",
      "% \\author{....lastname \\thanks{...} \\thanks{...} }\n",
      "%                     ^------------^------------^----Do not want these spaces!\n",
      "%\n",
      "% a space would be appended to the last name and could cause every name on that\n",
      "% line to be shifted left slightly. This is one of those \"LaTeX things\". For\n",
      "% instance, \"\\textbf{A} \\textbf{B}\" will typeset as \"A B\" not \"AB\". To get\n",
      "% \"AB\" then you have to do: \"\\textbf{A}\\textbf{B}\"\n",
      "% \\thanks is no different in this regard, so shield the last } of each \\thanks\n",
      "% that ends a line with a % and do not let a space in before the next \\thanks.\n",
      "% Spaces after \\IEEEmembership other than the last one are OK (and needed) as\n",
      "% you are supposed to have spaces between the names. For what it is worth,\n",
      "% this is a minor point as most people would not even notice if the said evil\n",
      "% space somehow managed to creep in.\n",
      "\n",
      "\n",
      "\n",
      "% The paper headers\n",
      "\\markboth{Journal of \\LaTeX\\ Class Files,~Vol.~14, No.~8, August~2015}%\n",
      "{Shell \\MakeLowercase{\\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}\n",
      "% The only time the second header will appear is for the odd numbered pages\n",
      "% after the title page when using the twoside option.\n",
      "% \n",
      "% *** Note that you probably will NOT want to include the author's ***\n",
      "% *** name in the headers of peer review papers.                   ***\n",
      "% You can use \\ifCLASSOPTIONpeerreview for conditional compilation here if\n",
      "% you desire.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% If you want to put a publisher's ID mark on the page you can do it like\n",
      "% this:\n",
      "%\\IEEEpubid{0000--0000/00\\$00.00~\\copyright~2015 IEEE}\n",
      "% Remember, if you use this you must call \\IEEEpubidadjcol in the second\n",
      "% column for its text to clear the IEEEpubid mark.\n",
      "\n",
      "\n",
      "\n",
      "% used for special paper notices\n",
      "%\\IEEEspecialpapernotice{(Invited Paper)}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% make the title area\n",
      "\\maketitle\n",
      "\n",
      "% As a general rule, do not put math, special symbols, or citations\n",
      "% in the abstract or keywords.\n",
      "\\begin{abstract}\n",
      "\\todo{still many items are WIP, but I can use some feedback.}\n",
      "Cutscenes are crucial in enhancing player immersion and storytelling in video games. They serve as the narrative backbone, seamlessly connecting gameplay with cinematic experiences. However, testing and validating cutscenes present a unique challenge due to their complex visual nature, which requires precise synchronization of in-game events, characters, and the environment.\n",
      "Traditional testing methods often fall short in addressing these complexities, leading to the need for a more efficient and accurate approach.\n",
      "In this paper, we introduce \\sceneGPT, a novel approach for generating test oracles for video game cutscenes.\n",
      "By leveraging the powerful capabilities of large pre-trained models, our method converts natural language descriptions of expected cutscene outputs into test oracles.\n",
      "\\sceneGPT facilitates customization of the testing procedure by incorporating human-in-the-loop feedback, allowing expert input to refine and adjust the generated oracles, ensuring accuracy and thoroughness. \n",
      "Qualitative findings indicate that \\sceneGPT successfully creates test oracles for a diverse range of real-world video game cutscene situations.\n",
      "Our approach holds the potential to considerably decrease the time and effort devoted to cutscene validation, ultimately enhancing the overall quality of video games.\n",
      "\\end{abstract}\n",
      "\n",
      "% Note that keywords are not normally used for peer-reviewed papers.\n",
      "\\begin{IEEEkeywords}\n",
      "Software testing and debugging, video games, bug reports, game testing, video game cinematics\n",
      "\\end{IEEEkeywords}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% For peer review papers, you can put extra information on the cover\n",
      "% page as needed:\n",
      "% \\ifCLASSOPTIONpeerreview\n",
      "% \\begin{center} \\bfseries EDICS Category: 3-BBND \\end{center}\n",
      "% \\fi\n",
      "%\n",
      "% For peer-review papers, this IEEEtran command inserts a page break and\n",
      "% creates the second title. It will be ignored for other modes.\n",
      "\\IEEEpeerreviewmaketitle\n",
      "\n",
      "\n",
      "\\section{Introduction}\n",
      "\n",
      "\\IEEEPARstart{O}ver the past few years, the video game industry has emerged as one of the most profitable segments in the entertainment market.\n",
      "The video game's revenues have surged at an impressive rate, reflecting its growing popularity among the masses.\n",
      "From a software engineering perspective, producing a video game involves a multitude of stages and components.\n",
      "As with any other software product, video games must undergo a rigorous testing process to guarantee the proper functioning of each component and the system as a whole.\n",
      "Nevertheless, testing video games presents a unique challenge due to the necessity for thorough visual inspection, ensuring that the final output meets the desired quality standards.\n",
      "This is distinct from conventional software products, which also might require UI testing but typically demand less extensive visual examination.\n",
      "\n",
      "In-game cinematics or cutscene is a video game sequence being shown to the player once they reach a particular point in the game.\n",
      "Cutscenes are used to create a more immersive experience for players by providing them with an engaging visual story.\n",
      "Cutscenes have also been frequently used to display conversations between characters in the game, providing a more realistic and interactive experience for the player. \n",
      "Testing cutscenes using traditional software tools is, however, a challenging task.\n",
      "Specifically, it is necessary to verify that all aspects, including characters, dialogue, and cinematography, are accurately depicted.\n",
      "\n",
      "\n",
      "In this study, we initially highlight the need for visual testing of video game cutscenes by conducting a preliminary study of Internet videos of cutscene bugs. \n",
      "Then, we present \\sceneGPT, a novel testing approach designed to evaluate the visual output of cutscenes by leveraging large pre-trained, or foundation, models.\n",
      "Our method involves transforming natural language descriptions of desired cutscenes into test specifications, which can then be assessed using an array of computer vision models to ensure accurate cutscene rendering.\n",
      "\n",
      "\n",
      "In summary, our contributions are as follows:\n",
      "\n",
      "\\begin{itemize}\n",
      " \\item We conduct an in-depth study of cutscene bugs in the wild and provide a categorization to facilitate a better understanding of potential issues related to cutscenes and their underlying causes.\n",
      " \\item We propose \\sceneGPT, a novel approach that leverages ChatGPT~\\cite{ChatGPT} and HuggingGPT~\\cite{shen2023hugginggpt} to convert natural language descriptions of a cutscene into test oracles.\n",
      " \\item We present a cutscene bug dataset, comprising bugs collected in real-world environments, to encourage further research on video game cutscenes and visual testing in general.\n",
      " \\item We extensively evaluate the performance of our proposed method using a real-world dataset.\n",
      "\\end{itemize}\n",
      "\n",
      "%-------------------------------------------------------------------------\n",
      "\\section{Background and Motivation}\n",
      "\n",
      "\n",
      "\\begin{figure*}[ht!]\n",
      "    \\centering\n",
      "    \\begin{minipage}[t]{0.24\\textwidth}\n",
      "        \\includegraphics[width=\\linewidth]{images/skyrim_bug/1.png}\n",
      "        \\label{fig:syrim_image1}\n",
      "    \\end{minipage}\\hfill\n",
      "    \\begin{minipage}[t]{0.24\\textwidth}\n",
      "        \\includegraphics[width=\\linewidth]{images/skyrim_bug/2.png}\n",
      "        \\label{fig:syrim_image2}\n",
      "    \\end{minipage}\\hfill\n",
      "    \\begin{minipage}[t]{0.24\\textwidth}\n",
      "        \\includegraphics[width=\\linewidth]{images/skyrim_bug/3.png}\n",
      "        \\label{fig:syrim_image3}\n",
      "    \\end{minipage}\\hfill\n",
      "    \\begin{minipage}[t]{0.24\\textwidth}\n",
      "        \\includegraphics[width=\\linewidth]{images/skyrim_bug/4.png}\n",
      "        \\label{fig:syrim_image4}\n",
      "    \\end{minipage}\n",
      "    \\caption{The infamous cutscene bug in Skyrim caused by a bee collision, resulting in a disrupted opening scene.}\n",
      "    \\label{fig:example_cutscene_skyrim}\n",
      "\\end{figure*}\n",
      "\n",
      "\n",
      "In video games, cutscenes serve an important role in advancing the game's story, providing context information about the game's world, and providing an immersive experience for the player.\n",
      "Cutscenes can take various forms, including (1) in-game engine cutscenes, (2) pre-rendered cinematic sequences, or (3) a combination of the two.\n",
      "In-game engine cutscenes are created using the game engine itself, allowing for greater control over the in-game assets and a more seamless transition between gameplay and cutscenes.\n",
      "However, creating and maintaining in-game engine cutscenes requires a great deal of software engineering expertise.\n",
      "Cutscenes need to be carefully scripted, and timed to ensure they integrate seamlessly with gameplay and do not cause any technical issues or glitches.\n",
      "More importantly, like any other part of the software, in-game cutscenes require continuous testing to ensure they function properly.\n",
      "\n",
      "\n",
      "\n",
      "The motivation behind implementing automated visual testing in game development lies in the pursuit of increased efficiency and reliability in guaranteeing the integrity and quality of in-game visuals.\n",
      "By automating this process, developers can both save time and resources and also minimize the risk of human error in the testing phase.\n",
      "Automated visual testing enables a more streamlined development process, allowing for rapid identification and correction of visual issues.\n",
      "Additionally, due to the consistent and precise nature of automated tests, they yield more accurate results compared to manual testing.\n",
      "\n",
      "Traditional software testing, a well-established method employed for years to verify software functionality, involves running code through a series of tests to detect issues or bugs.\n",
      "Test oracles serve as a mechanism for determining a software system's success or failure in a test by comparing expected and actual results.\n",
      "However, generating test oracles for visually rich software, like video games, presents a significant challenge. \n",
      "This stems from the fact that visual software applications heavily rely on the appearance and behavior of visual elements within the final rendered image.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The example provided\\footnote{Source: GamePhysics subreddit -- \\href{https://www.reddit.com/r/GamePhysics/comments/n6tu6e}{https://redd.it/n6tu6e}} in Figure \\ref{fig:example_cutscene_skyrim} demonstrates a well-known cutscene bug in Skyrim's opening scene, caused by a physics simulation error.\n",
      "When a bee unexpectedly collides with a cart being pulled by a horse, the game's physics engine inaccurately calculates the forces of the collision, leading to the cart shaking violently and the horse veering off course, which disrupts the entire sequence. \n",
      "A potential solution for developers to address this issue involves implementing automated tests on cutscene sequences.\n",
      "By running these tests multiple times and comparing the outcomes against a predefined specification, developers can effectively identify and rectify errors prior to the game's release, thereby enhancing its overall quality and stability.\n",
      "\n",
      "\n",
      "In this study, we argue that a synergistic approach integrating language and computer vision models can significantly improve the testing process for video game cutscenes. Using language models, expectations from a cutscene, such as \"this cutscene depicts a group of people riding in a cart pulled by a horse along a road,\" can be effectively broken down into a series of detailed requirements.\n",
      "Next, computer vision models can be employed to assess the integrity of a scene by determining whether the rendered frames align with the intended visual criteria.\n",
      "The combination of these models not only enhances the quality and stability of video game cutscenes but also effectively prevents the majority of cutscene bugs.\n",
      "\n",
      "\n",
      "\n",
      "%-------------------------------------------------------------------------\n",
      "\\section{Preliminary Study}\n",
      "\n",
      "\n",
      "In order to gain a comprehensive understanding of the intricacies of cutscene anomalies within video games, we have undertaken an exploratory investigation.\n",
      "The primary aim of this preliminary study is to establish a fundamental categorization of cutscene bugs, with the intention of elucidating their distinct types and variations.\n",
      "It is important to note that our classification methodology does not intend to provide a root-cause analysis; rather, it emphasizes the identification of the various manifestations of cutscene anomalies within the gaming environment.\n",
      "\n",
      "\n",
      "\\subsection{Data Collection}\n",
      "\n",
      "We construct a dataset comprising real-world cutscene bugs(i.e., ``bugs in the wild'') from YouTube videos.\n",
      "We employ YouTube's official search API to locate videos containing either the term ``cutscene bug'' or ``cutscene glitch'' that were published between 2015 and the end of 2022.\n",
      "The initial search yields 750 videos, a significant portion of which are unsuitable for our analysis.\n",
      "\n",
      "To refine our dataset, we randomly select 100 videos for examination, using this sample to establish exclusion criteria. We eliminate videos that: (1) do not pertain to game bugs, (2) feature 2D games, (3) exceed two minutes in duration, (4) include specific keywords in their titles, or (5) exhibit bugs unrelated to the cutscene.\n",
      "We use a range of keywords to filter out videos that are not suitable for our study.\n",
      "For example, the keywords \\textit{Hidden} and \\textit{Ending} usually refer to videos showing hidden cutscenes or how the game ends, and the keyword \\textit{Reaction} typically refers to videos with reactions from the player.\n",
      "Additionally, the keyword \\textit{Skip} usually refers to videos that explain tricks to skip parts of games.\n",
      "We use a comprehensive list of keywords to ensure that only relevant videos are included in our study.\n",
      "The complete list of keywords we used for this step is as follows: \\textit{Fix}, \\textit{Reaction}, \\textit{Complication}, \\textit{Hidden}, \\textit{Ending}, \\textit{Skip}, \\textit{Explained}.\n",
      "\n",
      "Upon applying our exclusion criteria, our dataset comprises 129 cutscene bugs distributed across 70 games that span a variety of genres and gameplay elements. Some samples for cutscene bugs are depicted in~\\ref{fig:sample_cutscene_bugs_perlim}.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{figure*}[ht!]\n",
      "    \\centering\n",
      "    \\begin{minipage}[t]{0.24\\textwidth}\n",
      "        \\includegraphics[width=\\linewidth]{images/sample_bugs/Bug1.png}\n",
      "        \\caption{A character is clipping through the door - RDR2}\n",
      "        \\label{fig:5}\n",
      "    \\end{minipage}\\hfill\n",
      "    \\begin{minipage}[t]{0.24\\textwidth}\n",
      "        \\includegraphics[width=\\linewidth]{images/sample_bugs/Bug2.png}\n",
      "        \\caption{A car is moving upside down in the street - The Crew}\n",
      "        \\label{fig:6}\n",
      "    \\end{minipage}\\hfill\n",
      "    \\begin{minipage}[t]{0.24\\textwidth}\n",
      "        \\includegraphics[width=\\linewidth]{images/sample_bugs/Bug3.png}\n",
      "        \\caption{Additional character being rendered in the scene - Defiance 2050}\n",
      "        \\label{fig:7}\n",
      "    \\end{minipage}\\hfill\n",
      "    \\begin{minipage}[t]{0.24\\textwidth}\n",
      "        \\includegraphics[width=\\linewidth]{images/sample_bugs/Bug4.png}\n",
      "        \\caption{An airliner crashes outside the window - GTA V}\n",
      "        \\label{fig:8}\n",
      "    \\end{minipage}\n",
      "\n",
      "    \\caption{Sample cutscene bugs}\n",
      "    \\label{fig:sample_cutscene_bugs_perlim}\n",
      "\\end{figure*}\n",
      "\n",
      "\n",
      "\n",
      "\\subsection{Constructing Categories of Bug Types}\n",
      "\n",
      "\n",
      "\n",
      "We use an iterative labeling process for categorizing cutscene bugs into seven distinct categories: `Animation', Audio', Camera', Logic', Performance', Physics', and Rendering'. Initially, we review each video and annotate them with simple tags that capture the specific aspects of the bug. For instance, if two objects are moving within each other, we label the video with a `clipping' tag. To create more comprehensive categories, we then merge related tags using a hierarchical clustering approach. For example, the tags `object falling through the ground' and `object shaking' will be combined to form a higher-level group called `Physics'. This process enables us to create semantically meaningful groups that better represent the nature of the bugs.\n",
      "\n",
      "Lastly, we assign a final label to each group that accurately encapsulates the primary characteristics of all bugs within that group. The labeling process focuses solely on the bug's manifestation in the cutscene, without relying on external descriptions or sources of information.\n",
      "\n",
      "\n",
      "\\begin{table*}[]\n",
      "\\centering\n",
      "\\caption{Categories of cutscene bugs identified in preliminary study}\n",
      "\\label{tab:taxonomy_table}\n",
      "\\resizebox{\\textwidth}{!}{%\n",
      "\\begin{tabular}{@{}>{\\bfseries}l p{14cm}@{}}\n",
      "\\toprule\n",
      "Category & \\multicolumn{1}{c}{\\textbf{Description}} \\\\ \\midrule\n",
      "Animation & Bugs related to animations, such as issues with positioning, size, timing, and states of objects, characters, and environments in the scene. \\\\ \\cmidrule(l){2-2} \n",
      "\\multicolumn{1}{r}{Examples} & \\begin{tabular}[c]{@{}l@{}}Incorrect body posture (e.g., character in T-pose)\\\\ Object misplacement or clipping\\\\ Inaccurate object or character state (e.g., closed door that should be open)\\end{tabular} \\\\ \\midrule\n",
      "Audio & Issues concerning audio and musical elements, including glitches and syncing errors. \\\\ \\cmidrule(l){2-2} \n",
      "\\multicolumn{1}{r}{Examples} & Missing dialogue audio \\\\ \\midrule\n",
      "Camera & Problems with camera parameters affecting the cutscene's viewing experience. \\\\ \\cmidrule(l){2-2} \n",
      "\\multicolumn{1}{r}{Examples} & Camera displaying an incorrect location \\\\ \\midrule\n",
      "Logic & Errors in non-player character (NPC) behavior during the cutscene, such as improper movements or actions. \\\\ \\cmidrule(l){2-2} \n",
      "\\multicolumn{1}{r}{Examples} & NPC interacting incorrectly with characters in the cutscene \\\\ \\midrule\n",
      "Performance & Performance issues, such as lagging or other technical malfunctions, that impair the cutscene's overall quality. \\\\ \\cmidrule(l){2-2} \n",
      "\\multicolumn{1}{r}{Examples} & Random stutters and frame drops \\\\ \\midrule\n",
      "Physics & Bugs concerning physics simulations of events occurring during the cutscene. \\\\ \\cmidrule(l){2-2} \n",
      "\\multicolumn{1}{r}{Examples} & Physics simulation causing object misplacement \\\\ \\midrule\n",
      "Rendering & Bugs in the rendering process leading to incorrect representation of characters, objects, or environments; often referred to as \"graphics glitches\" within the community. \\\\ \\cmidrule(l){2-2} \n",
      "\\multicolumn{1}{r}{Examples} & \\begin{tabular}[c]{@{}l@{}}Invisible objects or characters\\\\ Missing or flickering textures\\end{tabular} \\\\ \\bottomrule\n",
      "\\end{tabular}%\n",
      "}\n",
      "\\end{table*}\n",
      "\n",
      "\n",
      "\n",
      "\\subsection{Learning from Empirical Study of Video Game Cutscene Bugs}\n",
      "\n",
      "Through our empirical analysis of video game cutscene bugs, we have established the significance of the visual manifestation of these issues. We observed that even if the internal state of the game is incorrect during a cutscene bug occurrence, a visual indication is almost always present. This revelation emphasizes the importance of examining the rendered frame in order to effectively determine the presence of a bug in the game.\n",
      "\n",
      "For example, consider the case of GTA V, where there is a recurring bug involving the police entering the cutscene and shooting at the player character. In our analysis, we found that this issue is an example of a bug where an NPC (a non-playable character in the game) performs certain actions it is not supposed to do. The cause of this problem is that the cutscene trigger does not check whether the player is in the \\textit{wanted state} or not, leading to undesirable sequences of actions. In this case, not only is the internal state of the game incorrect but there are also extra objects or characters present in the scene.\n",
      "\n",
      "Drawing insights from our preliminary study, we propose \\sceneGPT, a method that emphasizes the visual output of games to offer a generic and game-independent approach for testing cutscenes. By focusing on the visual aspects of the rendered frame, similar to how human testers assess the game and report issues based on their perception of the visuals, \\sceneGPT aims to generate test oracles for cutscenes. This approach allows for efficient testing within a pipeline, bypassing specific game-engine-related complications, ultimately enhancing the detection of cutscene bugs, improving the overall gaming experience, and saving development time and resources.\n",
      "\n",
      "\n",
      "\n",
      "%-------------------------------------------------------------------------\n",
      "\\section{Method}\n",
      "\n",
      "\n",
      "Our proposed method comprises two stages, (1) generating test oracles and (2) executing test cases.\n",
      "In the first stage, we convert the natural language description of a cutscene, combined with a correctly rendered frame, into a series of structured tasks. \n",
      "The outcome of this stage is a test oracle in JSON format, which is then utilized in the second stage.\n",
      "During the test execution, the engine receives a test oracle and an output frame and verifies whether the rendered frame complies with the specifications outlined in the oracle.\n",
      "In the subsequent subsections, we will discuss each stage in greater detail.\n",
      "\n",
      "\n",
      "\\subsection{Phase 1 -- Test Oracle Generation}\n",
      "\n",
      "\n",
      "\\begin{figure*}[ht!]\n",
      "        \\includegraphics[width=1.0\\linewidth]{images/method/SceneDetectiveGPT.pdf}\n",
      "        \\caption{Method overview -- \\todo{ WIP } }\n",
      "        \\label{fig:overview}\n",
      "\\end{figure*}\n",
      "\n",
      "The first stage of our proposed method involves generating test oracles.\n",
      "In this stage, we closely follow the HuggingGPT~\\cite{shen2023hugginggpt} to first parse the natural language description of the cutscene, and subsequently decompose it into a series of tasks.\n",
      "We employ a language model, specifically ChatGPT, as the brain for this stage. ChatGPT's role is to provide a seamless interface between various components, including several computer vision models.\n",
      "\n",
      "The first stage consists of four separate steps, converting a cutscene's natural language description into a structured JSON file. We analyze and decompose the description into tasks, utilize ChatGPT for coordination, and compile the output into a JSON file representing the desired visual outcome. Refer to Figure~\\ref{fig:overview} for an overview of the process.\n",
      "\n",
      "\n",
      "\n",
      "\\subsubsection{Task Planning}\n",
      "\n",
      "\n",
      "In the task planning phase, ChatGPT breaks down high-level cutscene descriptions into a series of simple tasks.\n",
      "This allows for a seamless connection between each cutscene requirement and a corresponding computer vision model responsible for verifying that requirement.\n",
      "For example, if a person's presence is specified in the cutscene description, ChatGPT will link this requirement to an object detection model. \n",
      "\n",
      "\\subsubsection{Model Selection}\n",
      "\n",
      "In Step 1, a series of tasks is generated as output. Following this, Step 2 involves selecting suitable models for each task from a pool, primarily composed of models found on Hugging Face.\n",
      "Each model is paired with a description that provides information on its functionality and other relevant details.\n",
      "During this step, ChatGPT examines these descriptions to identify the most appropriate model for each specific task, ensuring optimal performance and results.\n",
      "\n",
      "\n",
      "\n",
      "\\subsubsection{Task Execution}\n",
      "\n",
      "In this step, after assigning a particular model to a task, we carry out the task execution by performing model inference.\n",
      "We utilize the same inference engine as HuggingGPT and store the results for each model.\n",
      "HuggingGPT enables hybrid inference, providing the flexibility to choose between running the model locally or utilizing Hugging Face's inference endpoints.\n",
      "This adaptability ensures effective task execution while allowing various processing and resource needs.\n",
      "\n",
      "\\subsubsection{Oracle Generation}\n",
      "\n",
      "In the final step, the requirements for the correct frame are formalized as a JSON file. This file contains information about the specific model to be applied, the expected output, and the order of execution for the models. This file serves as the desired specification for a valid test case.\n",
      "\n",
      "\n",
      "\n",
      "\\subsection{Phase 2 -- Test Case Execution}\n",
      "\n",
      "Phase 2 focuses on the implementation and execution of test cases during various stages of game development. Similar to other software development processes, any changes in the code base trigger a set of test cases to verify the software's correctness and stability at different stages. This testing approach can also be applied to video games. For instance, when a new build or version of a game under development becomes available, the game can be executed and the cutscenes recorded. Subsequently, test cases are run, and the test oracle generated by our proposed method is utilized to determine whether a test passes or fails, ensuring the game's quality and functionality.\n",
      "\n",
      "\n",
      "% In software engineering, a test oracle is a tool used to evaluate the correctness of the output of a software system under test. Creating a test oracle for graphically rich software, such as video game cutscenes, can be particularly challenging due to the complexity of the visual components.\n",
      "% A test oracle must accurately and comprehensively assess a combination of factors, including camera angles, object positions, lighting, and animations, to ensure that a cutscene is rendered correctly.\n",
      "% The combination of machine learning primitives introduced in Sec.~\\ref{sec:primitives} enables a potential way to write effective test oracles.\n",
      "\n",
      "%-------------------------------------------------------------------------\n",
      "\\section{Experiments}\n",
      "\n",
      "\n",
      "In this section, we conduct a series of tests to evaluate the effectiveness and utility of \\sceneGPT in generating software tests for video game cutscenes.\n",
      "Rather than relying on synthetics scenes within a game engine, we chose to examine real-world bugs occurring in AAA games to better understand the practical implications of our proposed method.\n",
      "\n",
      "\n",
      "\\subsection{Constructing a Dataset of Cutscenes}\n",
      "\n",
      "\n",
      "Our dataset of cutscene rendering variations contains 150 frame pairs, each consisting of a valid and an alternative version of the same scene. We have curated this dataset to encompass a broad range of bugs, from minor texture changes to significant discrepancies in the overall scene appearance. Notably, some frame pairs in the dataset feature both valid scenes, illustrating variations in object appearances or settings within the game.\n",
      "\n",
      "% Rather than relying on a synthetic dataset that lacks the true distribution of game cutscene bugs, we have collected all bugs from real videos on YouTube. We believe this dataset serves as a valuable resource for researchers examining the effects of rendering bugs on the visual experience of a cutscene.\n",
      "\n",
      "Each frame pair in our dataset contains two images: (1) an acceptable source  rendering of a frame, and (2) an alternative version of the same cutscene that has some variations due to either a bug or difference in the appearance of objects but is still a \\textit{valid} rendering within the game (e.g., different clothing). We refer to the first (valid) version as a ``reference'' rendering and the second image as an ``alternative'' rendering. Similar to the preliminary study, we search YouTube for videos of cutscene bugs using keywords. We also gather complete walkthrough videos for each game, as they can serve as a reference set.\n",
      "\n",
      "% We apply multiple matching and perspective transformation algorithms to find corresponding frames between videos, i.e., videos belonging to the same cutscene.\n",
      "\n",
      "To accurately match and align frames extracted from videos, we employ a multi-stage process. First, we use deep features to match all possible frame pairs between two videos. Then, we eliminate unsuitable candidates such as black frames by applying a threshold. To ensure perfect alignment of the two frames, we apply a perspective transformation. This step is crucial for two reasons: first, due to different video resolutions and aspect ratios, and second, because the combination of video encoding and some randomness in animations, like breathing, results in certain frames lacking an exact match.\n",
      "\n",
      "\n",
      "We utilize CLIP~\\cite{radford2021learning} with ViT-B/32 architecture to construct features from frames. First, we apply CLIP to each frame of the input videos to obtain embeddings.\n",
      "Next, we calculate all pairwise cosine similarity scores between all frames in the walkthrough and buggy gameplay video. In this process, similar frames will have cosine similarity close to 1, while dissimilar frames yield values near 0.\n",
      "We define some empirical thresholds to filter out low-quality candidate matches. Interestingly, cosine similarity values very close to 1 can also indicate that the two frames are very dark or even entirely black.\n",
      "Applying two threshold values can help eliminate this set of low-quality matches, but manual verification of frames is still required.\n",
      "\n",
      "\n",
      "% We utilize CLIP~\\cite{radford2021learning} with ViT-B/32 architecture to construct features from frames.\n",
      "% First, we apply the CLIP to each frame of the input videos to get embeddings, and then we calculate all pair cosine similarity scores between all frames in the walkthrough and buggy gameplay video.\n",
      "% In this process, similar frames will have cosine similarity close to 1 and dissimilar frames get values near 0. \n",
      "% We define some empirical thresholds  to filter out low-quality candidate matches.\n",
      "% Interestingly, cosine similarity very close to 1 could also indicate that the two frames are very dark or even completely black.\n",
      "% Applying two threshold values of \\todo{VALUE} and \\todo{VALUE} can help to eliminate this set of bad low-quality matches, but still, manual verification of frames is needed.\n",
      "\n",
      "\n",
      "% We apply a perspective transform using features extracted from the SuperGlue model~\\cite{sarlin2020superglue} to address the discrepancies causes by the difference in video resolutions.\n",
      "% Online gameplay videos usually have different rendering and recording resolutions, i.e., the game has been rendered with various rendering settings and then recorded as a video.\n",
      "% This sometimes causes drastic differences in the location of objects in each frame, even having two valid versions of the same cutscene. SuperGlue helps us to find correspondence points in two images.\n",
      "%  We then use these points to calculate the homography matrix, which is used to warp the image to the other image's resolution.\n",
      "%  This helps us to align the two images and make them look as if they were recorded with the same resolution.\n",
      "\n",
      "To address the discrepancies caused by differences in video resolutions, we apply a perspective transform using features extracted from SuperGlue~\\cite{sarlin2020superglue}. Online gameplay videos usually have varying rendering and recording resolutions, meaning the game has been rendered with different settings and then recorded as a video. This sometimes leads to drastic differences in the location of objects in each frame, even when having two valid versions of the same cutscene. SuperGlue assists us in finding correspondence points in two images. We then use these points to calculate the homography matrix, which is employed to warp one image to match the other's resolution. This helps us align the two images, making them appear as if they were recorded with the same resolution.\n",
      "\n",
      "\n",
      "Lastly, we inspect all frame pairs to exclude incorrect matches or frames that are severely distorted after perspective transformation. Our final dataset contains X pairs of valid renderings and Y frame pairs for the buggy renderings of a cutscene.\n",
      "\n",
      "\n",
      "\n",
      "% Finally, we inspect all frame pairs to exclude wrong matches or frames that are severely distorted after perspective transformation. Our final dataset contains \\todo{X} pairs of valid renderings and \\todo{Y} frame pairs for the buggy renderings of a cutscene. \n",
      "\n",
      "\n",
      "% \\subsection{Creating test oracles}\n",
      "\n",
      "% We develop a web application to facilitate the writing of test oracles. This web application is an interface to all machine learning primitives outlined in Sec~\\ref{sec:primitives} and allows the test writer to apply various models to an image and generate a list of expected results.\n",
      "\n",
      "\n",
      "% The process of writing a test oracle is as follows: First, the test writer uploads an image of the cutscene they want to test. The web application then processes the image using machine learning primitives, which include perceptual similarity score, object detection, segmentation, and pose estimation. The test writer can then select the primitives they want to apply to the image and specify the expected results they want to get. \n",
      "\n",
      "% Once the primitives are selected, the web application generates a list of expected results based on the selected models. The test writer can then review and modify the list of expected results before generating the final test oracle. The goal of designing such an interface is to reduce the time and effort required to write test oracles. An overview of the process is depicted in Fig \\todo{ADD FIGURE HERE}.\n",
      "\n",
      "\n",
      "% \\textbf{A word on the noisy output of vision models}: We gathered our videos from YouTube, and some of them have gone through severe video processing, posing a challenge for any object detection models. We add the ability to apply object size thresholds to remove very small objects, which might cause distraction and an unstable testing pipeline.  \\todo{Moreover, heavy video compression also causes degradation of similarity scores across all data points.}\n",
      "\n",
      "% \\subsection{Ablation studies}\n",
      "% \\todo{We swap Detic for YoloV8 [CITE ME] for person identification}\n",
      "% \\todo{We employ several ImageToText models to generate natural language descriptions for reference frames.}\n",
      "% \\todo{OWL-ViT}\n",
      "\n",
      "\n",
      "\n",
      "\\subsection{Qualitative Results}\n",
      "\n",
      "\\todo{I literally throw away 80\\% of the previous code I wrote. I need some time to make the tweaks and run some benchmarks.}\n",
      "\n",
      "\\todo{ --- let's talk about this --- other papers usually only show a few promising samples and some failure cases, they do not report exact numbers  because this number is related to the both performance of the model as well as the human input --- although we have a dataset of frame pairs, we do not have the GT description for cutscenes, we can populate this set ourselves, but people can complain! ---- in the other hand, if we just show samples, reviewers will point out that our hypothesis ``combination of LLM and Vision can generate test oracles'' is untested in this research''. \n",
      "}\n",
      "%-------------------------------------------------------------------------\n",
      "% \\section{Experimental Results}\n",
      "\n",
      "% \\todo{Qualitative Results?}\n",
      "\n",
      "% \\todo{78\\% pass rate for valid images, 72\\% catching buggy frames}\n",
      "\n",
      "% \\todo{low numbers in the pass rate for valid images is caused by a severe difference between image pairs in our dataset, which is actually the very hard cases}\n",
      "\n",
      "\n",
      "% \\todo{breakdown of accuracy for valid and buggy frame pairs}\n",
      "\n",
      "\n",
      "% \\todo{breakdown of the performance of different parts of test oracles}\n",
      "\n",
      "\n",
      "% \\todo{performance of Detic vs Yolo for person identification}\n",
      "\n",
      "\n",
      "% \\subsection{Ablation study results}\n",
      "\n",
      "%-------------------------------------------------------------------------\n",
      "\\section{Related Work}\n",
      "\n",
      "\\subsection{Large Language Models \\todo{and tool usage}}\n",
      "\n",
      "ChatGPT, GPT4, LLaMA, MultiModal Models --- Emergent Abilities are not impratant \n",
      "\n",
      "Instruction following models, Prompt Engineering, and others\n",
      "\n",
      "\\todo{vision primitives are not important here}\n",
      "\n",
      "\n",
      "--- A paragraph about `tool using' in LLMs and comparing them with us\n",
      "\n",
      "--- A paragraph about HuggingGPT and the difference between us and them\n",
      "\n",
      "--- ViperGPT is closely related and should be included here\n",
      "\n",
      "\n",
      "\\subsection{Video Game Bug Detection}\n",
      "\n",
      "Related work: \\cite{Taesiri2022CLIPMG, taesiri2022large}\n",
      "\n",
      "%-------------------------------------------------------------------------\n",
      "%-------------------------------------------------------------------------\n",
      "\\section{Threats to Validity}\n",
      "\n",
      "\n",
      "\\subsection{Threats to Internal Validity}\n",
      "\n",
      "\n",
      "-- Stochastic nature of deep learning models\n",
      "\n",
      "\n",
      "-- Reliability \n",
      "\n",
      "\n",
      "\\subsection{Threats to External Validity}\n",
      "\n",
      "-- Dataset \n",
      "\n",
      "Don't know the run time environment for these games. Possible tampering and modification done by the gamer.\n",
      "\n",
      "\n",
      "-- Distribution of bugs is not the same as the bugs in the company making the game\n",
      "\n",
      "%-------------------------------------------------------------------------\n",
      "\\section{Discussions and Conclusion}\n",
      "\n",
      "\\subsection{Negative impact of the dataset on the accuracy}\n",
      "\n",
      "The dataset is gathered from the internet and this imposes a massive challenge for all vision models. In a real testing environment, the amount of noise and variability in video quality can be controlled and thus we expect that the accuracy of our model goes up considerably.\n",
      "\n",
      "\\subsection{Limitations}\n",
      "\n",
      "\\begin{figure}[!]\n",
      "    \\centering % <-- added\n",
      "\\begin{subfigure}{0.48\\textwidth}\n",
      "  \\includegraphics[width=\\linewidth]{images/sample_bugs/mag_okay_Vncf_9LLagc.png}\n",
      "  \\caption{Normal}\n",
      "  \\label{fig:11}\n",
      "\\end{subfigure}\\hfil % <-- added\n",
      "\\begin{subfigure}{0.48\\textwidth}\n",
      "  \\includegraphics[width=\\linewidth]{images/sample_bugs/mag_buggy_Jxx2SW5mA.png}\n",
      "  \\caption{Bug}\n",
      "  \\label{fig:12}\n",
      "\\end{subfigure}\\hfil % <-- added\n",
      "\n",
      "\\end{figure}\n",
      "\n",
      "\n",
      "-- In this research, we focused on visual output and disregarded all audio-related bugs (e.g., lipsync issues, audio not being played, audio not in sync)\n",
      "\n",
      "-- The clipping bugs category is a challenge for existing off-the-shelf vision models. \n",
      "\n",
      "-- missing parts of the object: If some part of the object is not being rendered, it is usually a huge challenge to detect it properly\n",
      "\n",
      "--- does not discuss how test cases would be created, maintained, or updated throughout the game development process. It would be important to address the management of test cases, especially considering the rapidly changing nature of game development and the need to keep test cases relevant to the current build or version.\n",
      "\n",
      "\n",
      "\\todo{crowd system}\n",
      "\n",
      "\\todo{procedural generation -- animation or assets}\n",
      "\n",
      "\\todo{intractable cutscenes -- combat scenes}\n",
      "\n",
      "\n",
      "-- Lip sync\n",
      "\n",
      "-- Describe the texture\n",
      "\n",
      "-- Segmentation masks for Soft materials like clothing, whole vs parts\n",
      "\n",
      "\n",
      "In this paper, we introduce a novel approach for the visual testing of video games using recent advances in computer vision techniques.\n",
      "\n",
      "\n",
      "%-------------------------------------------------------------------------\n",
      "\\subsection{Broader Impact}\n",
      "\n",
      "The method proposed in this research is highly flexible and can be adapted to other parts of the game or even other graphically-rich applications with relative ease.\n",
      "We mainly focused on video game cutscenes, as they come with a predefined set of specifications that can be easily converted into test oracles. \\todo{Web development, android application testing, etc}\n",
      "\n",
      "\n",
      "\\todo{\n",
      "load the checkpoint, go to position X and see if there the object is  present at location Y\n",
      "}\n",
      "\n",
      "%-------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% \\section{Backlog}\n",
      "\n",
      "% \\begin{figure}\n",
      "% \\centering\n",
      "% \\subfloat[\\centering ]{%\n",
      "% \\begin{tikzpicture}\n",
      "% \\node[inner sep=0] (image) at (0,0) {\\includegraphics[width=1\\columnwidth]{images/shrinking_bug/juqt0n-0006.png }};\n",
      "% \\draw[red, thick] (0,1) rectangle (1.5,-2);\n",
      "% \\end{tikzpicture}\n",
      "% }%\n",
      "% \\qquad\n",
      "% \\subfloat[\\centering ]{%\n",
      "% \\begin{tikzpicture}\n",
      "% \\node[inner sep=0] (image) at (0,0) {\\includegraphics[width=1\\columnwidth]{images/shrinking_bug/juqt0n-0009.png }};\n",
      "% \\draw[red, thick] (0,1) rectangle (1.5,-2);\n",
      "% \\end{tikzpicture}\n",
      "% }%\n",
      "% \\caption{A sample of a cutscene-related bug -- In this bug, the character shrinks in size immediately after the cutscene's transition. -- \\todo{swap with Skyrim bug}} %\n",
      "% \\label{fig:example_cutscene_shrinks_backlog}\n",
      "% \\end{figure}\n",
      "\n",
      "% \\todo{In the given example\\footnote{Source: GamePhysics subreddit -- \\href{https://www.reddit.com/r/GamePhysics/comments/juqt0n}{https://redd.it/juqt0n}} shown in Figure \\ref{fig:example_cutscene_shrinks_backlog}, a cutscene bug is illustrated where the character's size suddenly and noticeably changes. \n",
      "% While this bug does not cause the game to crash, it severely disrupts the immersion of the game experience.\n",
      "% Such a bug is likely caused by changes made to other sections of the game's code, which consequently affects the functioning of previously working parts.\n",
      "% % This bug is most likely introduced by some changes to other parts of the game's code, causing the old functioning part to break. \n",
      "% In the software engineering field, regression tests are responsible for preventing such inconsistencies by making sure new changes do not break the old functioning parts.\n",
      "% \\todo{If we were to describe the test oracle as a natural language phrase, we would say ``this scene should contain two people with size $x$ and $y$ at location $p1$ and $p2$''}.\n",
      "% }\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% if have a single appendix:\n",
      "%\\appendix[Proof of the Zonklar Equations]\n",
      "% or\n",
      "%\\appendix  % for no appendix heading\n",
      "% do not use \\section anymore after \\appendix, only \\section*\n",
      "% is possibly needed\n",
      "\n",
      "% use appendices with more than one appendix\n",
      "% then use \\section to start each appendix\n",
      "% you must declare a \\section before using any\n",
      "% \\subsection or using \\label (\\appendices by itself\n",
      "% starts a section numbered zero.)\n",
      "%\n",
      "\n",
      "\n",
      "% \\appendices\n",
      "% \\section{Proof of the First Zonklar Equation}\n",
      "% Appendix one text goes here.\n",
      "\n",
      "% % you can choose not to have a title for an appendix\n",
      "% % if you want by leaving the argument blank\n",
      "% \\section{}\n",
      "% Appendix two text goes here.\n",
      "\n",
      "\n",
      "% use section* for acknowledgment\n",
      "% \\section*{Acknowledgment}\n",
      "\n",
      "\n",
      "% Can use something like this to put references on a page\n",
      "% by themselves when using endfloat and the captionsoff option.\n",
      "\\ifCLASSOPTIONcaptionsoff\n",
      "  \\newpage\n",
      "\\fi\n",
      "\n",
      "\n",
      "\n",
      "% trigger a \\newpage just before the given reference\n",
      "% number - used to balance the columns on the last page\n",
      "% adjust value as needed - may need to be readjusted if\n",
      "% the document is modified later\n",
      "%\\IEEEtriggeratref{8}\n",
      "% The \"triggered\" command can be changed if desired:\n",
      "%\\IEEEtriggercmd{\\enlargethispage{-5in}}\n",
      "\n",
      "% references section\n",
      "\n",
      "% can use a bibliography generated by BibTeX as a .bbl file\n",
      "% BibTeX documentation can be easily obtained at:\n",
      "% http://mirror.ctan.org/biblio/bibtex/contrib/doc/\n",
      "% The IEEEtran BibTeX style support page is at:\n",
      "% http://www.michaelshell.org/tex/ieeetran/bibtex/\n",
      "%\\bibliographystyle{IEEEtran}\n",
      "% argument is your BibTeX string definitions and bibliography database(s)\n",
      "%\\bibliography{IEEEabrv,../bib/paper}\n",
      "%\n",
      "% <OR> manually copy in the resultant .bbl file\n",
      "% set second argument of \\begin to the number of references\n",
      "% (used to reserve space for the reference number labels box)\n",
      "\n",
      "\\bibliographystyle{IEEEtran}\n",
      "\\bibliography{works_cited}\n",
      "\n",
      "\n",
      "\n",
      "% biography section\n",
      "% \n",
      "% If you have an EPS/PDF photo (graphicx package needed) extra braces are\n",
      "% needed around the contents of the optional argument to biography to prevent\n",
      "% the LaTeX parser from getting confused when it sees the complicated\n",
      "% \\includegraphics command within an optional argument. (You could create\n",
      "% your own custom macro containing the \\includegraphics command to make things\n",
      "% simpler here.)\n",
      "%\\begin{IEEEbiography}[{\\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}\n",
      "% or if you just want to reserve a space for a photo:\n",
      "\n",
      "\n",
      "% \\begin{IEEEbiography}{Mohammad Reza Taesiri}\n",
      "% Biography text here.\n",
      "% \\end{IEEEbiography}\n",
      "\n",
      "\n",
      "% \\begin{IEEEbiography}{Cor-Paul Bezemer}\n",
      "% Biography text here.\n",
      "% \\end{IEEEbiography}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "% that's all folks\n",
      "\\end{document}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the anthroptic client and the ContextualQA model\n",
    "client = anthropic.Client(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "\n",
    "\n",
    "def load_context(zip_file_path):\n",
    "    print(zip_file_path.name)\n",
    "    latex_source = process_tex_files_from_zip(zip_file_path.name)\n",
    "    model = ContextualQA(client, model=\"claude-v1.3-100k\")\n",
    "    model.load_text(latex_source)\n",
    "    return (\n",
    "        model,\n",
    "        [(f\"Load the zip file {zip_file_path.name}\",\"Paper loaded, Now ask a question.\")],\n",
    "    )\n",
    "\n",
    "\n",
    "def answer_fn(model, question, chat_history):\n",
    "\n",
    "    # if question is empty, tell user that they need to ask a question\n",
    "    if question == \"\":\n",
    "        chat_history.append((\"No Question Asked\", \"Please ask a question.\"))\n",
    "        return model, chat_history, \"\"\n",
    "\n",
    "    response = model.ask_question(question)\n",
    "\n",
    "    chat_history.append((question, response[0]['completion']))\n",
    "    return model, chat_history, \"\"\n",
    "\n",
    "def clear_context():\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/gradio/components.py:4650: UserWarning: The 'color_map' parameter has been deprecated.\n",
      "  warnings.warn(\"The 'color_map' parameter has been deprecated.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/gradio/components.py:163: UserWarning: Unknown style parameter: color_map\n",
      "  warnings.warn(f\"Unknown style parameter: {key}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/gs/ftfl94c117q3mq9rqwfnx1mc0000gn/T/gradio/0eda83c269cde323f479701b143f7c2902426f2c/MR - SceneDetectorGPT.zip\n",
      "/var/folders/gs/ftfl94c117q3mq9rqwfnx1mc0000gn/T/gradio/0eda83c269cde323f479701b143f7c2902426f2c/MR - SceneDetectorGPT.zip\n",
      "/var/folders/gs/ftfl94c117q3mq9rqwfnx1mc0000gn/T/gradio/5069cbfb5fae4af70755e28d67d493f8090a4d1a/MR - CLIPxGamePhysics Extension.zip\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# Explore ArXiv Papers in Depth with `claude-v1.3-100k` - Ask Questions and Receive Detailed Answers Instantly\")\n",
    "    gr.Markdown(\n",
    "        \"Dive into the world of academic papers with our dynamic app, powered by the cutting-edge `claude-v1.3-100k` model. This app allows you to ask detailed questions about any ArXiv paper and receive direct answers from the paper's content. Utilizing a context length of 100k tokens, it provides an efficient and comprehensive exploration of complex research studies, making knowledge acquisition simpler and more interactive. (This text is generated by GPT-4 )\"\n",
    "    )\n",
    "\n",
    "    with gr.Column():\n",
    "        with gr.Column():\n",
    "            paper_id_input = gr.File(label=\"Upload the zip file\", type=\"file\")\n",
    "            btn_load = gr.Button(\"Load Paper\")\n",
    "            qa_model = gr.State()\n",
    "\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot().style(color_map=(\"blue\", \"yellow\"))\n",
    "            question_txt = gr.Textbox(\n",
    "                label=\"Question\", lines=1, placeholder=\"Type your question here...\"\n",
    "            )\n",
    "            btn_answer = gr.Button(\"Answer Question\")\n",
    "\n",
    "            btn_clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    btn_load.click(load_context, inputs=[paper_id_input], outputs=[qa_model, chatbot])\n",
    "\n",
    "    btn_answer.click(\n",
    "        answer_fn,\n",
    "        inputs=[qa_model, question_txt, chatbot],\n",
    "        outputs=[qa_model, chatbot, question_txt],\n",
    "    )\n",
    "\n",
    "    question_txt.submit(\n",
    "        answer_fn,\n",
    "        inputs=[qa_model, question_txt, chatbot],\n",
    "        outputs=[qa_model, chatbot, question_txt],\n",
    "    )\n",
    "\n",
    "    btn_clear.click(clear_context, outputs=[chatbot])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
