{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Arxiv using `claude-v1.3-100k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import openai\n",
    "import time\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import gradio as gr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper - Get Arxiv Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_arxiv_source(paper_id):\n",
    "    url = f\"https://arxiv.org/e-print/{paper_id}\"\n",
    "\n",
    "    # Get the tar file\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Open the tar file\n",
    "    tar = tarfile.open(fileobj=io.BytesIO(response.content), mode=\"r\")\n",
    "\n",
    "    # Load all .tex files into memory, including their subdirectories\n",
    "    tex_files = {\n",
    "        member.name: tar.extractfile(member).read().decode(\"utf-8\")\n",
    "        for member in tar.getmembers()\n",
    "        if member.name.endswith(\".tex\")\n",
    "    }\n",
    "    # Load all .tex files into memory, including their subdirectories\n",
    "    tex_files = {\n",
    "        member.name: tar.extractfile(member).read().decode(\"utf-8\")\n",
    "        for member in tar.getmembers()\n",
    "        if member.isfile() and member.name.endswith(\".tex\")\n",
    "    }\n",
    "\n",
    "\n",
    "    # Pattern to match \\input{filename} and \\include{filename}\n",
    "    pattern = re.compile(r\"\\\\(input|include){(.*?)}\")\n",
    "\n",
    "    # Function to replace \\input{filename} and \\include{filename} with file contents\n",
    "    def replace_includes(text):\n",
    "        output = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                command, filename = match.groups()\n",
    "                # LaTeX automatically adds .tex extension for \\input and \\include commands\n",
    "                if not filename.endswith(\".tex\"):\n",
    "                    filename += \".tex\"\n",
    "                if filename in tex_files:\n",
    "                    output.append(replace_includes(tex_files[filename]))\n",
    "                else:\n",
    "                    output.append(f\"% {line} % FILE NOT FOUND\")\n",
    "            else:\n",
    "                output.append(line)\n",
    "        return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "    if \"main.tex\" in tex_files:\n",
    "        # Start with the contents of main.tex\n",
    "        main_tex = replace_includes(tex_files[\"main.tex\"])\n",
    "    else:\n",
    "        # No main.tex, concatenate all .tex files\n",
    "        main_tex = \"\\n\".join(replace_includes(text) for text in tex_files.values())\n",
    "\n",
    "    return main_tex\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Client(api_key=os.environ['ANTHROPIC_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualQA:\n",
    "    def __init__(self, client, model=\"claude-v1.3-100k\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.context = \"\"\n",
    "        self.questions = []\n",
    "        self.responses = []\n",
    "\n",
    "    def load_text(self, text):\n",
    "        self.context = text\n",
    "\n",
    "    def ask_question(self, question):\n",
    "        leading_prompt = \"Consider the text document below:\"\n",
    "        trailing_prompt = (\n",
    "            \"Now answer the following question, use Markdown to format your answer.\"\n",
    "        )\n",
    "        prompt = f\"{anthropic.HUMAN_PROMPT} {leading_prompt}\\n\\n{self.context}\\n\\n{trailing_prompt}\\n\\n{anthropic.HUMAN_PROMPT} {question} {anthropic.AI_PROMPT}\"\n",
    "        response = self.client.completion_stream(\n",
    "            prompt=prompt,\n",
    "            stop_sequences=[anthropic.HUMAN_PROMPT],\n",
    "            max_tokens_to_sample=6000,\n",
    "            model=self.model,\n",
    "            stream=False,\n",
    "        )\n",
    "        responses = [data for data in response]\n",
    "        self.questions.append(question)\n",
    "        self.responses.append(responses)\n",
    "        return responses\n",
    "\n",
    "    def clear_context(self):\n",
    "        self.context = \"\"\n",
    "        self.questions = []\n",
    "        self.responses = []\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        del state[\"client\"]\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "        self.client = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_to_pdf(question, answer, output_filename):\n",
    "    latex_template = r\"\"\"\n",
    "    \\documentclass{{standalone}}\n",
    "    \\usepackage[utf8]{{inputenc}}\n",
    "    \\usepackage{{amsmath}}\n",
    "    \\usepackage{{amssymb}}\n",
    "    \\usepackage{{hyperref}}\n",
    "    \\usepackage{{varwidth}}\n",
    "    \\usepackage{{adjustbox}}\n",
    "    \\begin{{document}}\n",
    "    \\begin{{adjustbox}}{{margin=5mm}}\n",
    "    \\begin{{varwidth}}{{\\linewidth}}\n",
    "    \\textbf{{Question:}} \\\\\n",
    "    {question} \\\\\n",
    "    \\textbf{{Answer:}} \\\\\n",
    "    {answer}\n",
    "    \\end{{varwidth}}\n",
    "    \\end{{adjustbox}}\n",
    "    \\end{{document}}\n",
    "    \"\"\"\n",
    "\n",
    "    answer = answer[0][\"completion\"]\n",
    "    latex_content = latex_template.format(question=question, answer=answer)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        tex_file = Path(temp_dir) / \"qa.tex\"\n",
    "        pdf_file = Path(temp_dir) / \"qa.pdf\"\n",
    "\n",
    "        with open(tex_file, \"w\") as f:\n",
    "            f.write(latex_content)\n",
    "\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"pdflatex\",\n",
    "                \"-interaction=nonstopmode\",\n",
    "                \"-output-directory\",\n",
    "                temp_dir,\n",
    "                tex_file,\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "\n",
    "        shutil.copy(pdf_file, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102541"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_source = download_arxiv_source(\"2203.11096\")\n",
    "len(latex_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% \\documentclass[sigconf,screen,natbib=false]{acmart}\n",
      "\\documentclass[sigconf,screen,authorversion,nonacm,natbib=false]{acmart}\n",
      "\n",
      "% \\acmConference[MSR 2022]{MSR '22: Proceedings of the 19th International Conference on Mining Software Repositories}{May 23â€“24, 2022}{Pittsburgh, PA, USA}\n",
      "\n",
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage[style=ACM-Reference-Format,backend=biber,defernumbers=false,sortcites=true]{biblatex}\n",
      "\\usepackage{subcaption}\n",
      "\\usepackage{multirow}\n",
      "\\usepackage{xspace}\n",
      "\\usepackage{listings}\n",
      "\\usepackage{booktabs}\n",
      "\n",
      "\\RequirePackage{fontawesome}\n",
      "\n",
      "\\usepackage{xcolor, soul}\n",
      "\\sethlcolor{yellow}\n",
      "\n",
      "% \\usepackage{hyperref}\n",
      "% \\hypersetup{\n",
      "%     colorlinks=true,\n",
      "%     % linkcolor=blue,\n",
      "%     % filecolor=magenta,      \n",
      "%     urlcolor=blue\n",
      "% }\n",
      "\n",
      "% \\DeclareUnicodeCharacter{0301}{*************************************}\n",
      " \n",
      "% \\lstset{\n",
      "%   backgroundcolor=\\color{white},\n",
      "%   extendedchars=true,\n",
      "%   basicstyle=\\footnotesize\\ttfamily,\n",
      "%   showstringspaces=false,\n",
      "%   showspaces=false,\n",
      "%   numbers=none,\n",
      "%   numberstyle=\\footnotesize,\n",
      "%   numbersep=9pt,\n",
      "%   tabsize=2,\n",
      "%   breaklines=true,\n",
      "%   showtabs=false,\n",
      "%   captionpos=b,\n",
      "%   frame=single,\n",
      "% }\n",
      "\n",
      "\\newcommand{\\reddit}[2]{\n",
      "    \\href{https://www.reddit.com/r/GamePhysics/comments/#1}{{\\smaller\\faExternalLink}\\xspace#2}%\n",
      "}\n",
      "\n",
      " \n",
      "% Title 1 . CLIP meets GamePhysics: Towards bug identification in gameplay videos using zero-shot transfer learning\n",
      "\n",
      "\\addbibresource{works_cited.bib}\n",
      "\n",
      "\n",
      "% FOR ARXIV\n",
      "% \\settopmatter{printacmref=false}\n",
      "% \\setcopyright{none}\n",
      "% \\renewcommand\\footnotetextcopyrightpermission[1]{}\n",
      "% \\pagestyle{plain}\n",
      "% % \\settopmatter{printacmref=false}\n",
      "% % \\makeatletter\n",
      "% % \\def\\@copyrightpermission{\\relax}\n",
      "% % \\makeatother\n",
      "% \\makeatletter\n",
      "% \\renewcommand\\@formatdoi[1]{\\ignorespaces}\n",
      "% \\makeatother\n",
      "\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "    \\title{CLIP meets GamePhysics: Towards bug identification in gameplay videos using zero-shot transfer learning}\n",
      "\\author{Mohammad Reza Taesiri}\n",
      "\\email{taesiri@ualberta.ca}\n",
      "\\affiliation{\n",
      "  \\institution{University of Alberta}\n",
      "  \\city{Edmonton}\n",
      "  \\state{AB}\n",
      "  \\country{Canada}\n",
      "}\n",
      "\\author{Finlay Macklon}\n",
      "\\email{macklon@ualberta.ca}\n",
      "\\affiliation{\n",
      "  \\institution{University of Alberta}\n",
      "  \\city{Edmonton}\n",
      "  \\state{AB}\n",
      "  \\country{Canada}\n",
      "}\n",
      "\\author{Cor-Paul Bezemer}\n",
      "\\email{bezemer@ualberta.ca}\n",
      "\\affiliation{\n",
      "  \\institution{University of Alberta}\n",
      "  \\city{Edmonton}\n",
      "  \\state{AB}\n",
      "  \\country{Canada}\n",
      "}\n",
      "    \\date{January 2022}\n",
      "\t\n",
      "    \\begin{abstract}\n",
      "% In this paper, we introduced a searching method to look inside game play videos. \\hl{It is a really cool paper!} \\hl{videos are under-explored} \\hl{talk about our system}\n",
      "\n",
      "\n",
      "Gameplay videos contain rich information about how players interact with the game and how the game responds.  Sharing gameplay videos on social media platforms, such as Reddit, has become a common practice for many players. Often, players will share gameplay videos that showcase video game bugs. Such gameplay videos are software artifacts that can be utilized for game testing, as they provide insight for bug analysis. Although large repositories of gameplay videos exist, parsing and mining them in an effective and structured fashion has still remained a big challenge. \n",
      "In this paper, we propose a search method that accepts any English text query as input to retrieve relevant videos from large repositories of gameplay videos. Our approach does not rely on any external information (such as video metadata); it works solely based on the content of the video. By leveraging the zero-shot transfer capabilities of the Contrastive Language-Image Pre-Training (CLIP) model, our approach does not require any data labeling or training. To evaluate our approach, we present the \\texttt{GamePhysics} dataset consisting of 26,954 videos from 1,873 games, that were collected from the GamePhysics section on the Reddit website. Our approach shows promising results in our extensive analysis of simple queries, compound queries, and bug queries, indicating that our approach is useful for object and event detection in gameplay videos. An example application of our approach is as a gameplay video search engine to aid in reproducing video game bugs. Please visit the following link for the code and the data: \\href{https://asgaardlab.github.io/CLIPxGamePhysics/}{https://asgaardlab.github.io/CLIPxGamePhysics/}\n",
      "\n",
      "% A demo of our approach can be found at the following link: \\href{https://clipxgamephysics.taesiri.com/}{https://clipxgamephysics.taesiri.com/}.\n",
      "\n",
      "\n",
      "% Furthermore, by pre-preprocessing the video embeddings and using Faiss to provide an efficient frame-search method, we show that CLIP can be applied to video datasets in general.\n",
      "\t\\end{abstract}\n",
      "\t\n",
      "    % \\begin{CCSXML}\n",
      "    % <ccs2012>\n",
      "    % <concept>\n",
      "    % <concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>\n",
      "    % <concept_desc>Software and its engineering~Software testing and debugging</concept_desc>\n",
      "    % <concept_significance>500</concept_significance>\n",
      "    % </concept>\n",
      "    % </ccs2012>\n",
      "    % \\end{CCSXML}\n",
      "    \\ccsdesc[500]{Software and its engineering~Software testing and debugging}\n",
      "    \n",
      "\t\\keywords{video mining, bug reports, software testing, video games}\n",
      "\t\n",
      "    \n",
      "\n",
      "    \\settopmatter{printfolios=true}\n",
      "\t\\maketitle\n",
      "\n",
      "\\section{Introduction} \\label{sec:intro}\n",
      "\n",
      "% The video game industry is one of the biggest industries in the world, with approximately 2.9 billion players and a market cap of 159.3 Billion US dollars~\\footnote{Video Game Industry Statistics, Trends and Data In 2021 - WePC -  \\href{https://www.wepc.com/news/video-game-statistics/}{https://www.wepc.com/news/video-game-statistics/}} . \n",
      "Video game development is a highly complex process.\n",
      "%, and despite many efforts to make it easier, creating a perfect video game is still a difficult challenge. \n",
      "There are many unique challenges when applying general software engineering practices in video game development~\\cite{politowski2020dataset, murphy2014cowboys, stacey2009temporal, petrillo2009went, lewis2010went}, including challenges in game testing.\n",
      "Manual testing is a widely accepted approach to game testing~\\cite{politowski2021survey, varvaressos2017automated, pascarella2018video}, however this manual process is slow and error-prone, and most importantly, expensive.  \n",
      "On the other hand, it is challenging to automate game testing~\\cite{lewis2011whats, pascarella2018video, santos2018computer} due to the unpredictable outputs of video games.\n",
      "Despite progress towards automated game testing methods~\\cite{davarmanesh2020automating, ling2020using, varvaressos2017automated, tuovenen2019mauto} and game testing tools~\\cite{juliani2018unity, zheng2019wuji, bergdahl2020augmenting, pfau2020dungeons}, new approaches to game testing must be researched. %are required.\n",
      "% this problem is far from solved. \n",
      "% there is much room for improvement in game testing.\n",
      "% games testing remains an expensive and \n",
      "% there are many opportunities for research in video game testing\n",
      "\n",
      "%The focus of our paper is creating an approach that enables game developers to search gameplay videos to help identify an example of a reported bug.\n",
      "The difficulty of game testing due to the unique nature of games calls for unique testing methodologies as well. \n",
      "For example, we could leverage the visual aspect of games in the testing process.\n",
      "Having a gameplay video is very helpful when trying to reproduce a bug in the development environment for further analysis, as bug reports often contain incomplete information~\\cite{bettenburg2008makes}.\n",
      "The ability to search a large repository of gameplay videos with a natural language query would be useful to help reproduce such bug reports.\n",
      "For example, in the game development domain, a bug report might state `my car is stuck on the rooftop' without any screenshot or video to show what is actually happening. \n",
      "A gameplay video search would allow game developers to find an example instance of a specific bug in the pile of gameplay videos from their playtesting sessions or the internet (e.g., YouTube, Twitch). \n",
      "% Gameplay videos are software artifacts that contain very detailed information about what is happening in the game and how the player interacts with the game environment.\n",
      "% Gameplay videos can be utilized in many ways, for example to analyze and reproduce bugs.\n",
      "\n",
      "Despite containing rich information, the challenges related to video parsing and understanding mean that gameplay videos are difficult to utilize.\n",
      "Manually identifying bug instances is time consuming, and there is limited prior research on automatic methods for mining large repositories of gameplay videos~\\cite{lin2019identifying, luo2019making}.\n",
      "% Developers often use urgent (post-release) updates to perform bug fixes~\\cite{lin2017studying}, \n",
      "%  one of the most popular topics in game development Q\\&A forums is bug reports~\\cite{kamienski2021empirical}, indicating a need for new automated tools for games testing.\n",
      "\n",
      "\\begin{figure}[!t]\n",
      "    \\centering\n",
      "    \\includegraphics[width=\\columnwidth]{images/sample_frame.jpeg}\n",
      "    % \\captionsetup{justification=centering}\n",
      "    \\caption{\\reddit{9rqabp}{Video} identified by our approach with the bug query \\textit{`A horse in the air'} for Red~Dead~Redemption~2.}\n",
      "    \\label{fig:search_example}\n",
      "\\end{figure}\n",
      "\n",
      "In this paper, we address the challenges of extracting useful information from large repositories of gameplay videos.\n",
      "We propose an approach for mining gameplay videos using natural language queries by leveraging the Contrastive Language-Image Pre-Training (CLIP) model~\\cite{radford2021learning} to identify similar text-image pairs without any additional training (i.e., zero-shot prediction). We leverage CLIP for videos by pre-processing the video frame embeddings and use Faiss~\\cite{JDH17} to perform a fast similarity search for the pairs of text queries and video frames.\n",
      "In our approach, we present two methods to aggregate across the similarity scores of each text-image pair to identify relevant videos.\n",
      "To evaluate our approach, we collected and prepared the \\texttt{GamePhysics} dataset, consisting of 26,954 gameplay videos that predominantly contain game physics bugs.\n",
      "We evaluate our approach with sets of simple queries, compound queries, and bug queries, and show that our approach can identify objects and (bug-related) events in large repositories of gameplay videos.\n",
      "%great potential as a search engine to extract valuable information from large repositories of gameplay videos.\n",
      "Figure~\\ref{fig:search_example} shows an example of a video that was identified by our approach when searching videos from the Red Dead Redemption~2 game using the bug query \\textit{`A horse in the air'}.\n",
      "The primary application of our approach is as a gameplay video search engine to aid in reproducing game bugs.\n",
      "With further work, e.g. setting thresholds to limit false positives, our approach could also be used as a bug detection system for video games.\n",
      "\n",
      "The main contributions of our paper are as follows:\n",
      "\\begin{itemize}\n",
      "    \\item We propose an approach to search for objects and events in gameplay videos using a natural language text query.\n",
      "    \\item We collect and prepare the \\texttt{GamePhysics} dataset, consisting of 26,954 gameplay videos from 1,873 games. \n",
      "    \\item We report results that demonstrate the promising performance of our approach in identifying game physics bugs through 3 experiments with simple, compound, and bug queries.\n",
      "\\end{itemize}\n",
      "\n",
      "The remainder of our paper is structured as follows. In Section~\\ref{sec:background}, we motivate our study by providing relevant background information. In Section~\\ref{sec:related}, we discuss related work. In Section~\\ref{sec:approach}, we present our approach to mining large repositories of gameplay videos. In Section~\\ref{sec:dataset}, we discuss collecting and pre-processing the \\texttt{GamePhysics} dataset. In Section~\\ref{sec:experiments} we detail our experiment setup, and in Section~\\ref{sec:results} we present our results. In Section~\\ref{sec:discussion} we provide discussion and insights on the performance of our approach. In Section~\\ref{sec:limitations} we outline limitations of our approach. In Section~\\ref{sec:threats} we address threats to validity. We conclude our paper in Section~\\ref{sec:conclusion}.\n",
      "\n",
      "\n",
      "% \\footnote{Subset: \\href{http://165.232.141.160:50002/GTAV-Videos.zip}{http://165.232.141.160:50002/GTAV-Videos.zip} (full dataset to be made available, e.g., on Zenodo, in final version of the paper due to its size)}\n",
      "\\section{Motivation and Background} \\label{sec:background}\n",
      "\n",
      "\\begin{figure*}[!t]\n",
      "\t\\centering\n",
      "\t\\begin{subfigure}[t]{\\columnwidth}\n",
      "\t\t\\centering\n",
      "\t\t\\includegraphics[width=\\textwidth]{images/bugs/4jirzj.jpeg}\n",
      "% \t\t\\captionsetup{justification=centering}\n",
      "\t\t\\caption{\\reddit{4jirzj}{Bug} in Grand Theft Auto~V. Car stuck in a tree after colliding.}\n",
      "\t\t\\Description{Fully described in the text.}\n",
      "\t\t\\label{fig:sample_bug_1}\n",
      "\t\\end{subfigure}\n",
      "\t\\hfill\n",
      "\t\\begin{subfigure}[t]{\\columnwidth}\n",
      "\t\t\\centering\n",
      "\t\t\\includegraphics[width=\\textwidth]{images/bugs/6652mm.jpeg}\n",
      "% \t\t\\captionsetup{justification=centering}\n",
      "\t\t\\caption{\\reddit{6652mm}{Bug} in The Elder Scrolls~V: Skyrim. Dragon stuck in the ground.}\n",
      "\t\t\\Description{Fully described in the text.}\n",
      "\t\t\\label{fig:sample_bug_2}\n",
      "\t\\end{subfigure}\n",
      "\t\\hfill\n",
      "\t\\begin{subfigure}[t]{\\columnwidth}\n",
      "\t\t\\centering\n",
      "\t\t\\includegraphics[width=\\textwidth]{images/bugs/bur1ke.jpeg}\n",
      "% \t\t\\captionsetup{justification=centering}\n",
      "\t\t\\caption{\\reddit{bur1ke}{Bug} in Red Dead Redemption~2. Incorrect sitting animation.}\n",
      "\t\t\\Description{Fully described in the text.}\n",
      "\t\t\\label{fig:sample_bug_3}\n",
      "\t\\end{subfigure}\n",
      "\t\\hfill\n",
      "\t\\begin{subfigure}[t]{\\columnwidth}\n",
      "\t\t\\centering\n",
      "\t\t\\includegraphics[width=\\textwidth]{images/bugs/kv41nk.jpeg}\n",
      "% \t\t\\captionsetup{justification=centering}\n",
      "\t\t\\caption{\\reddit{kv41nk}{Bug} in Cyberpunk~2077. Cars stuck together after colliding.}\n",
      "\t\t\\Description{Fully described in the text.}\n",
      "\t\t\\label{fig:sample_bug_4}\n",
      "\t\\end{subfigure}\n",
      "\t\\caption{Sample instances of game physics bugs.}\n",
      "\t\\Description{Fully described in the text.}\n",
      "\t\\label{fig:sample_bugs}\n",
      "\\end{figure*}\n",
      "\n",
      "\\subsection{Video game (physics) bugs} \\label{subsec:bugs}\n",
      "% In the software engineering domain, a bug typically has a straightforward definition: a manifestation of unexpected behavior or incorrect results in a software system. \n",
      "% In video games, designers try to create a new world or replicate the existing one. \n",
      "% Game designers will create every rule in this new virtual world. \n",
      "% Sometimes these rules contradict what we see in real life, but this is correct and an expected behavior since it was designed this way in the first place. \n",
      "% Defining a bug in a video game can be a very complicated task, and catching these bugs is even more challenging. \n",
      "% Cars usually don't fly in real life, but seeing a flying car in a game doesn't mean we found a bug.\n",
      "\n",
      "In this paper, we are interested in a specific category of bugs in video games that we call `game physics' bugs. \n",
      "Game physics bugs are not necessarily related to an inaccurate physics simulation.\n",
      "Many of these bugs are related to the faulty representation of game objects due to an error in the internal state of that object.\n",
      "A few sample instances of game physics bugs can be seen in Figure~\\ref{fig:sample_bugs}.\n",
      "In Figure~\\ref{fig:sample_bug_1}, a bug from Grand Theft Auto~V related to object collisions is shown.\n",
      "Figure~\\ref{fig:sample_bug_2} shows a bug from The Elder Scrolls~V: Skyrim, related to object clipping.\n",
      "In Figure~\\ref{fig:sample_bug_3}, a bug from Red Dead Redemption~2 related to ragdoll poses can be seen.\n",
      "Figure~\\ref{fig:sample_bug_4} shows a bug from Cyberpunk~2077, related to object collisions.\n",
      "% One of the leading causes behind these bugs is physics simulations; therefore, we call them `game physics' bugs. \n",
      "Identifying game physics bugs is challenging because we need to be able to extract specific, high-level (abstract) events from the gameplay videos, that are often similar to correct behaviour. \n",
      "\n",
      "% \\subsection{GamePhysics subreddit}\n",
      "% \\hl{not needed - remove candidate}\n",
      "% Reddit is a social network and discussion website. Users can post different types of content, such as texts, images, videos, and even links to other websites. After a user posts content, other people can vote and comment on it. To organize posts related to various categories and subjects, users can create special pages called `Subreddits.' Currently, the Reddit website has more than 2.8 Million different subreddits  \\footnote{10 Reddit Statistics You Should Know in 2021 \\url{https://www.oberlo.ca/blog/reddit-statistics}.}. Each subreddit can have a set of distinct rules to encourage people to only post in a particular format or only post a specific type of media. Content moderation on each subreddit is conducted by a set of community members, called \"Moderators\". These moderator users generally are not affiliated with Reddit, and usually, they are selected from the members of the subreddit with a high level of activity and contribution.\n",
      "% %~\\cite{lin_2021}\n",
      "% In 2012, a subreddit called `GamePhysics' was created for sharing videos and gifs related to video games. All of the posts in this subreddit must address an event in a video game that is either a bug, funny, weird, or, in some cases, very accurate physics simulation in a video game. Currently, this subreddit has 71,885 submissions, 594,202 comments.\n",
      "\n",
      "\\subsection{Challenges in mining gameplay videos} \\label{subsec:challenges}\n",
      "Until now, it has been challenging to extract valuable information from large repositories of gameplay videos.\n",
      "Identifying bug instances by manually checking the contents of gameplay videos is time-consuming ~\\cite{lin2019identifying}.\n",
      "Therefore, automatic methods for mining gameplay videos are required.\n",
      "% Previous work has shown that it is feasible to automatically identify gameplay videos on social media that contain a bug using video metadata (such as keywords)~\\cite{lin2019identifying}, but automatically retrieving specific objects or events (such as bug instances) from gameplay videos requires a different approach.\n",
      "The only existing approach for automatically extracting events from gameplay videos requires manual data labelling (and the training of new models)~\\cite{luo2019making}, which itself is time-consuming.\n",
      "Therefore, an effective method for extracting valuable information from gameplay videos should be able to automatically analyze the video contents without requiring manual data labelling.\n",
      "\n",
      "% \\subsection{Contrastive learning and zero-shot transfer}\n",
      "% Zero-shot learning (ZSL) is a family of problems in machine learning, in which an algorithm is required to solve a task without having a training set for that specific task \n",
      "% ~\\cite{5206594, larochelle2008zero}. \n",
      "% To illustrate this idea, let us say that a person has never seen a zebra before, but if we give a detailed description of a zebra to them (e.g. an animal similar to a horse, but with black-and-white stripes all over their bodies), that person can identify zebras when they see one. \n",
      "% While there are many approaches towards zero-shot learning, we are interested in assessing the performance of pre-trained contrastive models. \n",
      "% Contrastive learning is a machine learning technique in which the goal is to learn a representation of inputs such that similar items stay close to each other in the learned space, while the dissimilar items are far away~\\cite{becker1992self, bromley1993signature}. \n",
      "% In recent years, contrastive learning has been one of the key drivers in the success of self-supervised learning methods, and has been used for zero-shot transfer learning~\\cite{chen2020simple, grill2020bootstrap, khosla2020supervised, radford2021learning}.\n",
      "% %but we are interested in the zero-shot capability of pre-trained models. \n",
      "% Contrastive models are suitable for our problem, as they learn to keep similar items together for both image and text inputs.\n",
      "% Although videos are more complex than images, we can simply treat each gameplay video as a collection of video frames, and can therefore utilize contrastive learning methods for video frames with natural language text queries.\n",
      "\n",
      "\\subsection{Contrastive learning and zero-shot transfer} \\label{subsec:contrastive}\n",
      "While there are many approaches towards zero-shot learning, we are interested in assessing the zero-shot performance of pre-trained contrastive models. \n",
      "Contrastive learning is a machine learning technique in which the goal is to learn a representation of inputs such that similar items stay close to each other in the learned space, while the dissimilar items are far away~\\cite{becker1992self, bromley1993signature}.\n",
      "In recent years, contrastive learning has been one of the key drivers in the success of self-supervised learning methods and has been used for zero-shot transfer learning~\\cite{chen2020simple, DBLP:conf/nips/GrillSATRBDPGAP20, khosla2020supervised, radford2021learning}. \n",
      "Zero-shot learning is a family of problems in machine learning, in which an algorithm is required to solve a task without having a training set for that specific task \n",
      "~\\cite{5206594, larochelle2008zero}. \n",
      "To illustrate this idea, suppose that a person has never seen a zebra before. \n",
      "If we give a detailed description of a zebra to them (e.g., an animal similar to a horse, but with black-and-white stripes all over their bodies), that person can identify a zebra when they see one.\n",
      "%\\hl{recent studies have shown that contrastive models can leverage both text and image inputs together}\n",
      "% Inputs to contrastive models can be of different types (e.g., text and image), meaning such a model can compare its representation of an image with its representation of some text.\n",
      "% \\hl{can probably reduce this}\n",
      "% Contrastive models learn to reduce the distance of similar objects, even they are of different types. \n",
      "% For instance, it is possible to train a model to measure the similarity of any arbitrary text input with an image. \n",
      "% This connection between images and texts is at the core of our search approach.\n",
      "% The reason behind this is that we treated each video as a collection of frames.\n",
      "\n",
      "\\subsection{The Contrastive Language-Image Pre-Training (CLIP) model} \\label{subsec:clip}\n",
      "% \\subsection{The CLIP model} \\label{subsec:clip}\n",
      "One contrastive model that has proven zero-shot transfer capabilities is the Contrastive Language-Image Pre-Training (CLIP) model~\\cite{radford2021learning}, which can leverage both text and image inputs together. We decided to use CLIP because of its multimodal capabilities and the size of its training dataset. The CLIP model consists of two parts: a text encoder, and an image encoder. \n",
      "These two parts work individually, and they can accept any English text and image as input. \n",
      "When an encoder of this model receives an input, it will transform it into an embedding vector. \n",
      "These embedding vectors are high-level features that are extracted by the network, representing the input. \n",
      "More specifically, these embedding vectors are how the neural network represents, distinguishes, and reasons about different inputs. \n",
      "Both encoders of this model will produce vectors of the same dimension for image and text inputs. \n",
      "Not only do these vectors have the same dimension, but they are also in the same high-dimensional feature space, and are therefore compatible with each other. \n",
      "For example, the embedding vector of the text `an apple' and the embedding vector of an image of an apple are very close to each other in this learned space.\n",
      "The CLIP model was pre-trained on over 400 million pairs of images and text descriptions that were scraped from the internet, and has six different backbone architectures: `RN50', `RN101', `RN50x4', `RN50x16', `ViT-B/32', `ViT-B/16'. \n",
      "The models with `RN' in their name are ResNet-based~\\cite{he2016deep} models using traditional convolutional layers, while the `ViT' models are based on vision transformers~\\cite{dosovitskiy2020image}. \n",
      "\n",
      "\n",
      "\\section{Related Work} \\label{sec:related}\n",
      "Event extraction from video content is of special importance for various data mining tasks~\\cite{macleod2015code, ponzanelli2016too}. Only two prior studies have explicitly explored automatic approaches for mining gameplay videos, with varying success. Lin et al. showed that using metadata (such as keywords) to identify YouTube videos that contain video game bugs is feasible~\\cite{lin2019identifying}, but our approach looks at the video contents, which Lin et al. do not take into account.\n",
      "Our approach is more useful for game developers, as we can identify objects and (bug-related) events within gameplay videos.\n",
      "Luo et al. propose an approach for automatic event retrieval in e-sport gameplay videos that requires manual data labelling, a fixed set of classes (events), and the training of new models~\\cite{luo2019making}.\n",
      "Our approach is more robust and easier to set-up, as we can search gameplay videos with any English text query to identify specific objects and events without performing manual data-labelling.\n",
      "\n",
      "Although there is limited prior work on mining large repositories of gameplay videos, there are several studies that propose approaches to automatically detect graphics defects in video games.\n",
      "% Our work falls into the category of automated bug detection for video games. \n",
      "% A bug in a video game can appear in different forms, either in visual representation or logical states. \n",
      "One of the earliest approaches for automated detection of graphics defects was published in 2008, in which a semi-automated framework was proposed to detect shadow glitches in a video game using traditional computer vision techniques~\\cite{nantes2008framework}. \n",
      "% With the recent advancements in deep learning and computer vision, convolutional neural networks (CNNs) have become the best candidates for image similarity and quality assessment~\\cite{zhang2018unreasonable}. \n",
      "Recent studies have utilized convolutional neural networks in their approach to automatically detect a range of graphics defects~\\cite{davarmanesh2020automating, ling2020using, taesirivideo, chen2021glib}.\n",
      "Instead of detecting graphics defects, our work is concerned with the automatic identification of game physics bugs in gameplay videos.\n",
      "\n",
      "Tuovenen et al. leverage the visual aspect of games through an image matching approach to create a record-and-replay tool for mobile game testing~\\cite{tuovenen2019mauto}.\n",
      "Our approach leverages the visual aspect of games in a different way; instead of recording tests through gameplay, we automatically identify bugs in gameplay videos.\n",
      "\n",
      "Some studies have proposed approaches for automated detection of video game bugs through static or dynamic analysis of source code.\n",
      "Varvaressos et al. propose an approach for runtime monitoring of video games, in which they instrument the source code of games to extract game events and detect injected bugs~\\cite{varvaressos2017automated}.\n",
      "Borrelli et al. propose an approach to detect several types of video game-specific bad smells, which they formalize into a tool for code linting~\\cite{borrelli2020detecting}. \n",
      "Our approach differs as we do not require access to the source code of games; instead we identify video game bugs based solely on the contents of gameplay videos.\n",
      "\n",
      "In addition to related work on automatic bug detection for video games, there exists a wide range of work that leverages recent advancements in deep learning to provide new tools and techniques that address problems faced by game developers.\n",
      "Several studies have sought to make AI methods accessible in the video game development and testing cycle, either through the game's internal state, raw pixels, or through a high-level neural network-based representation~\\cite{khameneh2020embedding, trivedi2021contrastive}. \n",
      "% After this step, it is possible to utilize wast range of approaches and techniques to improve game design, debugging, and testing. \n",
      "Some studies have proposed approaches to accompany a game designer through the creation process of a game by providing suggestions and explanations to the designer~\\cite{guzdial2016game, DBLP:conf/aiide/GuzdialLR18, khadivpour2020responsibility}. \n",
      "Other studies have incorporated reinforcement learning and evolutionary methods to create AI agents that can automatically play games~\\cite{justesen2019deep, berner2019dota, vinyals2019grandmaster}. \n",
      "These AI agents can be further employed to perform automated game testing sessions~\\cite{roohi2021predicting, 9619048, 9231552, zheng2019wuji, GARCIASANCHEZ2018133}.\n",
      "Our work is different from those listed above, as we focus on assisting game developers by providing an approach to efficiently search large repositories of gameplay videos to find bug instances.\n",
      "\n",
      "% Many attempts have been made \\cite{davarmanesh2020automating, ling2020using, varvaressos2017automated, tuovenen2019mauto} to make the game testing a more automated process. Besides this automation goal, some tools have been designed to make game testing less challenging \\cite{juliani2018unity, zheng2019wuji, bergdahl2020augmenting, pfau2020dungeons}.\n",
      "\n",
      "% Some of our experiments have been directed towards bug identification in video games, using keywords for specific events in the game. In \\cite{lin2019identifying} an approach proposed to determine if a gameplay video on social media contains bugs or not. This approach only works on metadata and completely ignores the content of the video. Our search system incorporates every frame and pixel of video to make a prediction.\n",
      "% The closest previous work to this paper is \\cite{luo2019making} in which the authors have proposed training a convolutional neural network to identify sequence events in e-sport video games. \n",
      "% Our proposed system is far more flexible and requires no training stage out of the box. \n",
      "% Our work also works for any arbitrary text query, in contrast to  \\cite{luo2019making} which is only able to classify a limited set of existing predefined events. \n",
      "\n",
      "% One category of bug detection systems focuses on graphical issues in the game. First attempts for automated graphical glitch detection in video game dates back to 2008. In \\cite{nantes2008framework} a semi-automated framework was proposed to detect shadow glitches in a video game using traditional computer vision techniques. With the recent advancements in deep learning and computer vision, convolutional neural networks (CNNs) have become the best candidate for image similarity and quality assessment \\cite{zhang2018unreasonable}. A pretrained CNN can be fine-tuned to identify a wide range of graphical issues from the pixels of a frame \\cite{luo2019making, taesirivideo}.\n",
      "\\section{Our Approach} \\label{sec:approach}\n",
      "To assist with detection and analysis of game bugs, we propose an approach that quickly and effectively searches a large repository of gameplay videos to find a specific object or event in a particular game.\n",
      "For creating such a powerful search system, one could utilize a traditional supervised classification technique. \n",
      "However, any supervised classification method requires a training dataset, a test dataset, and a fixed number of classes. \n",
      "Maintaining these two datasets and labeling each sample is demanding and labour-intensive.\n",
      "On the other hand, the CLIP model provides zero-shot transfer learning capabilities that allow us to develop an approach to automatically mine gameplay videos while avoiding the aforementioned issues. % faced when using supervised learning approaches.\n",
      "Figure \\ref{fig:videosearch} shows an overview of our approach.\n",
      "\n",
      "\\subsection{Encoding video frames and the text query} \\label{subsec:encoding}\n",
      "Our approach accepts a set of videos and any English text query as inputs.\n",
      "We first extract all frames from each video, and then use the CLIP model to transform our input text query and input video frames into the embedding vector representations described in Section~\\ref{subsec:clip}.\n",
      "% need to restructure below a little\n",
      "% The CLIP model is the key component in our approach, as it can connect natural language text to an image, allowing us to compare text queries with video frames to see if they are similar (or not).\n",
      "We selected the CLIP model because it is flexible enough to accept any arbitrary English text as a query and compare it with a video frame, without any additional training.\n",
      "\n",
      "\\subsection{Calculating the similarity of embeddings} \\label{subsec:similarity}\n",
      "As well as avoiding manual data labelling, our approach avoids depending upon any extra information, such as metadata, to search gameplay videos.\n",
      "Instead, we are able to calculate similarity scores solely based on the contents of the video frames and the text query.\n",
      "The similarity score in our problem is a distance between an embedding vector representing a text query and another embedding vector representing a video frame.\n",
      "% A higher similarity score corresponds to a lower distance between the text and image embedding vectors.\n",
      "To calculate similarity scores for the pairs of embedding vectors, we opted for cosine similarity, a widely-used similarity metric ~\\cite{viggiato2021identifying, 10.1007/978-3-030-41131-2_3, fazli2021under, viggiatousing}. % for high-dimensional vectors.\n",
      "% While it is possible to utilize Euclidean distance (L2 norm) for this problem, \n",
      "% The cosine similarity between two vectors is defined as the inner-product of the vectors normalized by their length.\n",
      "% Suppose the two input vectors have the same direction (very similar).\n",
      "% In that case, the cosine similarity becomes $1$, and when the two vectors are perpendicular to each other (not similar), this value becomes $0$.\n",
      "% \\subsection{Large-scale similarity search using FAISS}\n",
      "% \\hl{can be merged with prev paragraph to save space}\n",
      "% \\subsubsection{Efficient large-scale similarity search using Faiss}\n",
      "% % In the search process, we are performing a similarity search on a massive number of data points in a very high-dimensional space. \n",
      "% As explained in Section~4.3, we require an exhaustive search to calculate the similarity score of the text query with each individual frame in each input video.\n",
      "% The performance of an exhaustive search will decrease inversely with an increasing number of videos in a repository. The search process essentially is the problem of finding the nearest neighbors in a high-dimensional vector space. To make this process faster, we make clusters on the frame's embeddings. These clusters will group similar frames, which helps perform the search process more structured manner. To this end, we use Faiss~\\cite{JDH17} to conduct an efficient search.\n",
      "% For example, we can cut the search from 10 seconds to just a few milliseconds; for all 2,230 videos from \\textit{Grand Theft Auto~V} in our \\texttt{GamePhysics} dataset.\n",
      "We require an exhaustive search to calculate the similarity score of the text query with each individual frame in each input video. \n",
      "The performance of an exhaustive search will decrease inversely with an increasing number of videos in a repository. \n",
      "To combat this, we use Faiss~\\cite{JDH17} to conduct an efficient similarity search.\n",
      "\n",
      "\\subsection{Aggregating frame scores per video} \\label{subsec:aggregating}\n",
      "Although CLIP is designed to accept text and images as inputs, we can leverage CLIP for videos by treating each video as a collection of video frames (i.e. a collection of images). \n",
      "To identify specific events that could occur at any moment in a gameplay video, we cannot subsample the video frames as suggested in the original CLIP, because due to the richness of events in a single gameplay video,  skipping any part of the video may lead to information loss and inaccurate results. \n",
      "Therefore, we perform a similarity search on all frames of all videos by comparing each individual video frame with the target query text, and we subsequently aggregate the similarity scores across each video.\n",
      "Below we detail the design of two different methods for aggregating the video frame similarity scores for each gameplay video.\n",
      "Our approach supports the two aggregation methods without the need to re-calculate the similarity scores. \n",
      "\n",
      "\\subsubsection*{Aggregating frame scores using the maximum score}\n",
      "Our first score aggregation method ranks videos based on the maximum similarity score across all frames belonging to each video.\n",
      "This method is highly sensitive, as a single frame with high similarity can lead to an entire video being identified as relevant to the query.\n",
      "\n",
      "\\subsubsection*{Aggregating frame scores using the similar frame count}\n",
      "In the second score aggregation method, we begin by ranking all frames of the input videos based on their similarity scores with the text query.\n",
      "Then, we select a predefined number (the \\emph{pool size} hyperparameter) of highest-ranked frames across all videos. \n",
      "Finally, we count the number of frames per video within this pool of highest-ranked frames. \n",
      "%reduce the list of all ranked frames to a set of only the most highly ranked frames.\n",
      "%to restrict the number of highly ranked frames used for selecting matching videos.\n",
      "%We rank videos based on their frame count in our reduced set of highly ranked frames.\n",
      "This method is less sensitive than our first aggregation method, as identified videos must have multiple frames that are among the most similar to the input text query.\n",
      "We selected 1,000 as the default pool size value in our study.\n",
      "\n",
      "%have the highest count of frames among all frames with the highest similarity to the input text query.\n",
      "% To better reference each of the aggregations, we will use `\\textbf{Top-$k$}' and `\\textbf{Majority Voter}' when referring to the first and the second aggregation method, respectively. \n",
      "\n",
      "\\begin{figure}[!t]\n",
      "\\centering\n",
      "\\includegraphics[width=\\linewidth]{images/search_overview.pdf}\n",
      "\\caption{Overview of our gameplay video search approach.}\n",
      "\\label{fig:videosearch}\n",
      "\\end{figure}\n",
      "\n",
      "% \\subsection{Selecting top-$k$ videos}\n",
      "% Because our approach relies on zero-shot prediction, i.e., the videos are not already labelled with ground truth, we select the top-$k$ ranked input videos as our predictions for each query.\n",
      "% We chose 5 as our default value for $k$.\n",
      "% This means that our search approach returns the 5 gameplay videos which are most similar to the input text query, without ever having to manually analyze or label the videos.\n",
      "\n",
      "% \\hl{kinda bullshitting but want to tie our prediction back to zero-shot, and make it clear that we can accept some false positives with our approach.}\n",
      "\n",
      "% One way of retrieving videos based on the similarity scores is to pick any video that contains a frame with high similarity to the input text query.\n",
      "% On other words, if only a single frame in a video has the highest overall similarity to the query, that video will be chosen as the prediction of the system. \n",
      "% For some specific tasks, this is a desirable property, if we want our approach to be extremely sensitive, e.g. detecting something that may occur in only one frame. \n",
      "% But, this information aggregation method is susceptible to noise as well, for example a video compression artifact might cause CLIP to produce an inaccurate embedding representation for a frame.\n",
      "% % To have an \\hl{adaptive?} system for both of the \\hl{conditions?} mentioned above, we created two separate score aggregation mechanisms.\n",
      "% Therefore, we use two different aggregation methods across the video frames to make the final prediction for each text query: k-NN, and majority vote.\n",
      "\n",
      "% \\begin{figure}[!h]\n",
      "% \\centering\n",
      "% \\includegraphics[width=8cm]{images/TSNE.pdf}\n",
      "% \\includegraphics[width=8cm]{images/PACMAP.pdf}\n",
      "% \\caption{Sample Explanations}\n",
      "% \\end{figure}\n",
      "\n",
      "% \\begin{figure}[!h]\n",
      "% \\centering\n",
      "% \\includegraphics[width=8cm]{images/TSNE.pdf}\n",
      "% \\includegraphics[width=8cm]{images/PACMAP.pdf}\n",
      "% \\caption{TSNE and PACMAP}\n",
      "% \\end{figure}\n",
      "\n",
      "\\section{Preparing the GamePhysics dataset} \\label{sec:dataset}\n",
      "\n",
      "\\subsection{Collecting the \\texttt{GamePhysics} dataset} \\label{subsec:collecting}\n",
      "\n",
      "Developing and testing a new machine learning system requires a dataset. Unfortunately, there is no such dataset for gameplay bugs. To this end, we present the \\texttt{GamePhysics} dataset, which consists of \\textbf{26,954} gameplay videos collected from the \\href{https://www.reddit.com/r/GamePhysics/}{{\\smaller\\faExternalLink} GamePhysics} subreddit. \n",
      "An overview of our data collection process can be seen in Figure~\\ref{fig:datacollection}.\n",
      "\n",
      "\\begin{figure}[!t]\n",
      "\\centering\n",
      "\\includegraphics[width=\\linewidth]{images/data_collection.pdf}\n",
      "\\caption{Overview of our data collection process.}\n",
      "\\label{fig:datacollection}\n",
      "\\end{figure}\n",
      "\n",
      "\\subsubsection*{Extracting post metadata and downloading videos}\n",
      "To collect the data, we created a custom crawler that uses both the official Reddit API and the popular \\texttt{PushShift.io} API \\cite{baumgartner2020pushshift}. \n",
      "In our crawler, we use the \\texttt{PushShift.io} API to get high-level information about each submission in the GamePhysics subreddit. \n",
      "After obtaining high-level data, we use Reddit's official API to update the scores and other metadata of each submission. \n",
      "For downloading the actual video files, we use a combination of \\texttt{youtube-dl} and \\texttt{aria2c} to extract video links and download them.\n",
      "\n",
      "\\subsubsection*{Filtering posts}\n",
      "We applied several filters to our dataset during the data collecting process to remove spam posts, low-quality content, and outliers. \n",
      "There are several spam posts in the GamePhysics subreddit, and these posts are marked explicitly as spam by the subreddit's moderators. \n",
      "Furthermore, we treat post scores as a quality signal as this score captures up/down votes from Reddit users, and consider any post with a score of less than one as low-quality content. \n",
      "The lengths of the video files vary from a few seconds to multiple hours. \n",
      "We avoid long videos in our dataset, because they can contain multiple events of different kinds and are very hard to process. \n",
      "We only keep videos that are longer than 2 seconds and shorter than 60 seconds. \n",
      "After applying our filters, our final dataset contains \\textbf{26,954} video files from \\textbf{1,873} different games.\n",
      "\n",
      "\\subsubsection*{Labelling videos with the game name}\n",
      "In order to simulate the realistic scenario in which a game developer would search a repository of gameplay videos for a specific game, we extract the game name for each gameplay video from the title of its respective post.\n",
      "Detecting the game's name from a GamePhysics submission is not straightforward. \n",
      "While there is a community guideline that suggests including the name of the game in the submission's title, people often forget to include the game name or use several aliases for the game name, meaning the task of detecting the game name can be hard. \n",
      "For example, `GTA V' is a widely-used alias that refers to the `Grand Theft Auto V' game. \n",
      "To address this issue, we created a second custom crawler to search game name keywords in Google and subsequently map them to the full game name. \n",
      "Google search results provide a specific section called the Knowledge Panel that contains the game name, as well as other relevant game information such initial release date, genre, development studio(s), and publisher.\n",
      "\n",
      "\\subsection{Pre-processing the videos} \\label{subsec:preprocessing}\n",
      "As discussed in Section~\\ref{subsec:similarity}, our approach can search a large repository of gameplay videos more efficiently by pre-processing the embedding vectors of every frame for each video in the repository before inputting any text queries. \n",
      "Therefore, for our dataset to be suitable for our approach, we pre-process all videos in the \\texttt{GamePhysics} dataset before proceeding with any experiments.\n",
      "We pre-processed all 26,954 videos using a machine with two NVIDIA Titan RTX graphics cards, but it is certainly possible to perform this step with less powerful graphics cards too.\n",
      "It is worth noting that this is by far the most computationally expensive step in our approach.\n",
      "\\section{Experiment setup} \\label{sec:experiments}\n",
      "In this section, we describe an extensive analysis of our approach on the \\texttt{GamePhysics} dataset through a diverse set of experiments.\n",
      "To assess the performance of our video search method, we performed several experiments with varying levels of difficulty. % \\hl{complexity(?)}. \n",
      "The main obstacle to evaluating our search system is the lack of a benchmark dataset. \n",
      "To this end, we designed three experiments with three corresponding sets of queries to shed light on the capabilities of our proposed method.\n",
      "\n",
      "\\subsection{Experiment overview} \\label{subsec:expoverview}\n",
      "% We designed our experiments such that we are simulating the most realistic scenarios; developers of a certain game will search videos of only their game.\n",
      "% We make this design choice in our experiments to assess if our approach yields actionable results for game developers.\n",
      "% We assess the accuracy of our approach in finding specific objects and (bug-related) events in the dataset. \n",
      "In the first two experiments, we evaluate the accuracy of our approach when retrieving videos with certain objects in them. \n",
      "The results for this step indicate the generalization capability of the model for the third experiment.\n",
      "%and also the utility of our approach in a broader sense. \n",
      "% This evaluation is like a sanity check for our proposed approach. \n",
      "% Any system that can answer complex queries must also answer simpler ones. \n",
      "In the third experiment, we evaluate the accuracy of our approach when retrieving videos with specific events related to bugs.\n",
      "\n",
      "\\subsection{Selecting CLIP architectures} \\label{subsec:selectingclips}\n",
      "To understand the relative performance of the available ResNet-based and vision transformer-based CLIP models, we opted to try two different backbone architectures in our system, namely `RN101' and `ViT-B/32'. \n",
      "We chose these backbones as fair baseline comparisons because they are the largest backbone architectures in their respective families, assuming we stipulate equivalent input image sizes ($224\\times224$).\n",
      "For comparison, the `ViT-B/32` backbone architecture contains 151,277,313 total parameters, while the `RN101` backbone architecture contains 119,688,033 total parameters.\n",
      "We selected the largest architectures as we are performing inference with these models, not training them.\n",
      "\n",
      "\\begin{table*}[!t]\n",
      "    \\centering\n",
      "    \\caption{Games selected for evaluation of our approach. All selected games are open-world.}\n",
      "    \\begin{tabular*}{\\textwidth}{l @{\\extracolsep{\\fill}} l @{\\extracolsep{\\fill}} l @{\\extracolsep{\\fill}} l @{\\extracolsep{\\fill}} l @{\\extracolsep{\\fill}} r}\n",
      "        \\toprule\n",
      "        \\textbf{Game} & \\textbf{Key} & \\textbf{Genre} & \\textbf{Visual style} & \\textbf{Reason for inclusion} & \\textbf{Videos} \\\\\n",
      "        \\midrule\n",
      "        Grand Theft Auto V         & \\texttt{GTA} & Action-adventure & Realism & Variety of vehicles & 2,230 \\\\\n",
      "        Red Dead Redemption 2      & \\texttt{RDR} & Action-adventure & Realism & Historical style & 754 \\\\\n",
      "        Just Cause 3               & \\texttt{JC3} & Action-adventure & Realism & Physical interactions & 680 \\\\\n",
      "        Fallout 4                  & \\texttt{F4} & Action role-playing-game & Fantasy realism (Retro-futuristic) & Unique look and feel & 614 \\\\\n",
      "        Far Cry 5                  & \\texttt{FC5} & First-person shooter & Realism & First-person camera & 527 \\\\\n",
      "        Cyberpunk 2077             & \\texttt{C77} & Action-adventure & Fantasy realism (Futuristic) & High-quality lighting & 511 \\\\\n",
      "        The Elder Scrolls V: Skyrim & \\texttt{ESV} & Action role-playing-game & Fantasy realism & Magical effects & 489 \\\\\n",
      "        The Witcher 3: Wild Hunt   & \\texttt{W3} & Action role-playing-game & Fantasy realism & Mythical beasts & 387 \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular*}\n",
      "    \\label{tab:selected_games}\n",
      "\\end{table*}\n",
      "\n",
      "\\subsection{Selecting video games} \\label{subsec:selectinggames}\n",
      "Our dataset contains videos from 1,873 different video games, and the differences in their high-level characteristics, such as genre, visual style, game mechanics, and camera view, can be vast. \n",
      "Therefore, we performed a comprehensive evaluation in all three experiments with 8 popular video games that differ in their high-level characteristics.\n",
      "% Our main criterion when selecting these games is ensure we maximize the diversity of visual style within the set of selected video games.\n",
      "The only uniting characteristic for our selected games is that they have open-world mechanics, because developers of open-world games would find particular benefit from an effective video search for bug reproduction.\n",
      "Open-world games allow a player to freely explore the game world, providing a larger set of potential interactions between the player and game environment.\n",
      "Open-world games are therefore more likely to suffer from game physics bugs that are difficult to reproduce.\n",
      "Table~\\ref{tab:selected_games} shows each game we selected for our experiments, as well as some game characteristics and the reason for inclusion.\n",
      "% We selected eight video games from the top 10 most popular video games in our dataset. \n",
      "In total, 23\\% of videos in the \\texttt{GamePhysics} dataset are from these 8 video games (6,192 videos).\n",
      "% \\hl{Why not just top 10? -> Selected eight of the most popular (open-world) games, each with distinct combinations of genres and (art) styles.}\n",
      "% We picked games with a western theme, magical elements and dragons, contemporary shooters, and sci-fi elements.  \n",
      "% Since we have very different video games, we designed queries per video game (e.g. there is no car crash in a historical video game).\n",
      "% Due to the massive amount of interactions in \\textit{Just Cause 3}, familiar objects can often have adversarial poses \\cite{alcorn2019strike}. \n",
      "% By adversarial pose, we mean the object's relative position to the camera will fool the neural network and cause incorrect classification. \n",
      "% Due to this problem, this video game can be very challenging for any machine learning-based object classifier and detector, and is therefore a particularly interesting candidate for our experiments. \n",
      "% \\textit{Fallout 4} has a retrofuturism style that distinguishes itself from other games in our dataset. \n",
      "% The objects in this game are futuristic from the perspective of the 1950s. \n",
      "% Each frame of the gameplay videos from this video game can be considered as an out-of-distribution sample for our model. \n",
      "% The \\textit{Cyberpunk 2077} game offers ray-tracing for more realistic lighting. \n",
      "% \\textit{Skyrim} \\hl{(and \\textit{Witcher 3}?)} is challenging for most image classification and object recognition methods due to the fact that the visual style of the game is mildly different from natural image datasets. \n",
      "% Furthermore, it contains many animals, beasts, and creatures that do not exist in real life. \n",
      "\n",
      "\\subsection{Query formulation} \\label{subsec:formulation}\n",
      "To come up with a set of relevant search queries in the experiments, we randomly picked 10 videos from each of the 8 selected games. \n",
      "The first author manually reviewed each of the 80 samples to understand what was happening and how we could describe events in gameplay videos that contain bugs.\n",
      "This sampling process helped us pick relevant objects and events to use in our queries.\n",
      "\n",
      "\\subsection{Experiment 1: Simple Queries} \\label{subsec:exp1}\n",
      "In this experiment, we searched for specific objects in videos, e.g. a car.\n",
      "Our main objective in this experiment is to demonstrate the capability of our system for effective zero-shot object identification. As a reminder, we never trained or fine-tuned our neural network model for any of these experiments or any video game.  \n",
      "% For calculating the system's accuracy in this part, we only look at the top-1 results from our approach. i.e. we only care about the highest ranked video. \n",
      "% The first author manually analyzed each search result, and only accepted the results if the first retrieved video contains the object specified in the simple query. \n",
      "We created 22 distinct queries for Experiment~1, including transportation vehicles, animals, and special words describing the weather or environment. \n",
      "% Since some objects do not appear in every video game, we exclude them in the search process for that specific game (e.g., historical games do not have automobiles, and not all shooter games have horses).\n",
      "% We do not apply any thresholds in our approach, meaning our approach always returns the highest ranked video(s) from the set of input videos.\n",
      "% Therefore, we would not observe meaningful results when searching games that do not contain the objects of a particular query.\n",
      "% This is a reasonable decision in our experiment, as game developers would clearly tailor their queries to the game they are searching.\n",
      "For this experiment we wanted our approach to operate with very high sensitivity, and so we selected our first aggregation method, i.e. using maximum frame score per video (Section~\\ref{subsec:aggregating}).\n",
      "\n",
      "\\subsection{Experiment 2: Compound Queries} \\label{subsec:exp2}\n",
      "Continuing our evaluation, we search for compound queries, i.e. queries in which an object is paired with some descriptor.\n",
      "Similar to Experiment~1, we only use compound queries that are relevant to each video game.\n",
      "For example, in the previous experiment, we searched for videos in the Grand Theft Auto~V game that contained a car, but in this experiment we evaluate the performance of our approach when searching for objects with a specific condition, like a car with a particular color. \n",
      "For this second experiment, we created a set of 22 compound queries, and again selected our first aggregation method (using maximum frame score per video).\n",
      "\n",
      "\\subsection{Experiment 3: Bug Queries} \\label{subsec:exp3}\n",
      "In the third experiment, we search for bug queries, i.e. phrases that describe an event in the game which is related to a bug. \n",
      "We manually created specific textual descriptions for different bug behaviors in the game and searched our dataset to see if we could detect bugs in gameplay videos. \n",
      "Similar to the previous experiments, our bug queries are game-specific. \n",
      "For this experiment, we created a set of 44 unique bug queries, with each query describing an event.\n",
      "Given the complex nature of the bug queries in Experiment~3, we decided to use our less sensitive aggregation method, based on the number of highly similar frames per video (as described in Section~\\ref{subsec:aggregating}).\n",
      "\n",
      "\\subsection{Evaluating the experiments} \\label{subsec:evaluating}\n",
      "% In order to simulate the most realistic scenarios in which a game developer will use our approach to search their gameplay videos, we select appropriate accuracy metrics on a per experiment basis.\n",
      "% The first author manually analyzed each search result, and only accepted a result as correct if the retrieved video(s) contained the object specified in the simple query. \n",
      "\n",
      "% The first two experiments are designed to assess the sensitivity of our system in identifying objects in the gameplay videos. \n",
      "% We search the dataset with textual descriptions to identify game objects. \n",
      "% Since we want to determine the sensitivity of our system in these experiments, we used the `\\textbf{Top-$k$}' aggregator in the last phase of the search process \\hl{citation?}. \n",
      "\n",
      "% Similar to experiment 1, we used the \\textbf{`Top-$k$'} aggregator, but we incorporated a different strategy to calculate the system accuracy. \\hl{don't do this - confusing. instead use same metric for all experiments.}\n",
      "% To calculate the system's accuracy in this experiment, we look at the top 5 results of the system, and we accept the answer if either of these videos satisfies the query we searched for. We relaxed the acceptance criteria for this experiment because compound queries are much more challenging than simple queries.\n",
      "\n",
      "\\subsubsection*{Evaluating Experiment 1 and Experiment 2.}\n",
      "In the first and second experiments, we assess the sensitivity of our approach by measuring top-$1$ and top-$5$ accuracy.\n",
      "This is because for our approach to be useful to a game developer, the search system should be able to reliably identify objects specified in the text queries.\n",
      "Top-$k$ accuracy is a binary measure; if there is a correct result in the top-$k$ results, the accuracy is $100\\%$, otherwise the accuracy is $0\\%$ -- there are no possible values in between. \n",
      "%Top-$1$ accuracy is $100\\%$ if the single top ranked result (with highest similarity score) for a text query is correct, otherwise it is $0\\%$.\n",
      "%Top-$5$ accuracy is less strict; it is measured as $100\\%$ if any of the top five ranked videos for a text query is correct, otherwise it is $0\\%$.\n",
      "\n",
      "% If the top-$k$ returned result(s) for a simple or compound query in a game (that is known to have the specified object) is incorrect, we assume that our approach failed to return the correct video(s).\n",
      "\n",
      "% % \\subsubsection{Evaluating experiment 2}\n",
      "% We evaluate the second experiment similar to the first, as this experiment is still concerned with the object identification capabilities of our approach. We also considered both top-$1$ and top-$5$ accuracy for this experiment. Since we search for objects with specific conditions in this experiment, we expect lower values for top-$1$.\n",
      "\n",
      "\n",
      "\\subsubsection*{Evaluating Experiment 3}\n",
      "In the third experiment, we measured the accuracy of our approach using recall~@5.\n",
      "The reason for this choice is that we want to see what proportion of videos are relevant to the bug query, and how susceptible our system is to false positives when searching with bug queries. \n",
      "It is possible to report recall at higher levels, but the problem is that we cannot know how many videos in the dataset exactly match the search query without manually checking every video.\n",
      "Recall~@5 is $100\\%$ when all five out of five retrieved videos match the bug query, $80\\%$ when four out of five retrieved videos match, etc. until $0\\%$ when there are no matching videos.\n",
      "% We designed the third experiment such that it was explicitly related to bug identification in video games.\n",
      "% We created a set of special queries that either describe a bug in a video game or are related to an event that is likely to be a bug. \n",
      "% Unlike the first two experiments, we want our model to be less sensitive to the noise; therefore, we used the `\\textbf{Majority Voter}' aggregation method \\hl{citation?}.\n",
      "% \\hl{why not use both aggregation methods and describe which is the best?}\n",
      "\n",
      "\\section{Results} \\label{sec:results}\n",
      "In this section, we present the results of the three experiments we designed to examine the ability of our proposed search system. \n",
      "% The first two experiments were created as a sanity check to confirm if our approach can reliably identify objects in gameplay videos. \n",
      "% The third experiment tested the performance of our approach when searching for videos with specific bug-related events.\n",
      "\n",
      "% \\subsection{Summary of the results}\n",
      "% For \\textit{Fallout 4}, although both models are often struggling to find relevant videos, they still achieve modest performance. \n",
      "% For \\textit{Far Cry 5}, both models on this game perform similarly well, but it struggles particularly when finding videos related to `helicopters'. \n",
      "\n",
      "\\begin{table}[t!]\n",
      "\\centering\n",
      "\\caption{Average top-$k$ accuracy (\\%) per game for simple queries (Experiment~1).}\n",
      "\\label{tab:pergameresults1}\n",
      "\\begin{tabular*}{\\linewidth}{l@{\\extracolsep{\\fill}}l@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r}\n",
      "\\toprule\n",
      " {} & \\textbf{} & \\textbf{{\\texttt{GTA}}} & \\textbf{{\\texttt{RDR}}} & \\textbf{{\\texttt{JC3}}} & \\textbf{{\\texttt{F4}}} & \\textbf{{\\texttt{FC5}}} & \\textbf{{\\texttt{C77}}} & \\textbf{{\\texttt{ESV}}} & \\textbf{{\\texttt{W3}}} \\\\\n",
      " \\midrule\n",
      "\\textbf{ViT-B/32} & \\textbf{Top-$1$} & 74 & 71 & 61 & 65 & 50 & 55 & 54 & 54 \\\\\n",
      " & \\textbf{Top-$5$} & 89 & 86 & 67 & 71 & 88 & 73 & 62 & 62 \\\\\n",
      " \\midrule\n",
      "\\textbf{RN101} & \\textbf{Top-$1$} & 84 & 50 & 61 & 59 & 59 & 43 & 62 & 62 \\\\\n",
      " & \\textbf{Top-$5$} & 89 & 79 & 83 & 82 & 94 & 71 & 92 & 85 \\\\\n",
      " \\bottomrule\n",
      "\\end{tabular*}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[t!]\n",
      "\\centering\n",
      "\\caption{Average top-$k$ accuracy (\\%) per game for compound queries (Experiment~2).}\n",
      "\\label{tab:pergameresults2}\n",
      "\\begin{tabular*}{\\linewidth}{l@{\\extracolsep{\\fill}}l@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r}\n",
      "\\toprule\n",
      "{} & \\textbf{} & \\textbf{\\texttt{GTA}} & \\textbf{\\texttt{RDR}} & \\textbf{\\texttt{JC3}} & \\textbf{\\texttt{F4}} & \\textbf{\\texttt{FC5}} & \\textbf{\\texttt{C77}} & \\textbf{\\texttt{ESV}} & \\textbf{\\texttt{W3}} \\\\\n",
      "\\midrule\n",
      "\\textbf{ViT-B/32} & \\textbf{Top-$1$} & 68 & 88 & 56 & 43 & 31 & 50 & 56 & 56 \\\\\n",
      " & \\textbf{Top-$5$} & 100 & 100 & 81 & 64 & 69 & 75 & 89 & 67 \\\\\n",
      "\\midrule\n",
      "\\textbf{RN101} & \\textbf{Top-$1$} & 84 & 88 & 31 & 36 & 56 & 67 & 33 & 44 \\\\\n",
      " & \\textbf{Top-$5$} & 95 & 100 & 75 & 79 & 94 & 83 & 78 & 56 \\\\        \n",
      " \\bottomrule\n",
      "\\end{tabular*}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[t!]\n",
      "    \\caption{Average top-$k$ accuracy (\\%) per query for simple queries (Experiment~1). $N$ is the number of games searched.}\n",
      "    \\label{tab:results_t1}\n",
      "    \\centering\n",
      "    \\begin{tabular*}{\\linewidth}{l @{\\extracolsep{\\fill}} r r @{\\extracolsep{\\fill}} r r @{\\extracolsep{\\fill}} r}\n",
      "    \\toprule\n",
      "    \\textbf{} & \\textbf{} & \\multicolumn{2}{l}{\\textbf{ViT-B/32}} & \\multicolumn{2}{l}{\\textbf{RN101}} \\\\\n",
      "    \\textbf{Query} & \\textbf{$N$} & \\textbf{Top-$1$} & \\textbf{Top-$5$} & \\textbf{Top-$1$} & \\textbf{Top-$5$} \\\\\n",
      "    \\midrule\n",
      "    \\textit{Airplane} & 4 & 75 & 100 & 100 & 100 \\\\\n",
      "    \\textit{Bear} & 5 & 80 & 100 & 60 & 100 \\\\\n",
      "    \\textit{Bike} & 4 & 50 & 75 & 50 & 100 \\\\\n",
      "    \\textit{Bridge} & 8 & 88 & 88 & 50 & 100 \\\\\n",
      "    \\textit{Car} & 5 & 80 & 100 & 80 & 100 \\\\\n",
      "    \\textit{Carriage} & 4 & 50 & 50 & 75 & 100 \\\\\n",
      "    \\textit{Cat} & 6 & 33 & 50 & 33 & 67 \\\\\n",
      "    \\textit{Cow} & 8 & 63 & 75 & 25 & 75 \\\\\n",
      "    \\textit{Deer} & 7 & 57 & 71 & 75 & 100 \\\\\n",
      "    \\textit{Dog} & 8 & 25 & 38 & 38 & 63 \\\\\n",
      "    \\textit{Fire} & 8 & 88 & 100 & 100 & 100 \\\\\n",
      "    \\textit{Helicopter} & 5 & 60 & 60 & 60 & 100 \\\\\n",
      "    \\textit{Horse} & 3 & 67 & 100 & 100 & 100 \\\\\n",
      "    \\textit{Mountain} & 7 & 100 & 100 & 100 & 100 \\\\\n",
      "    \\textit{Parachute} & 2 & 0 & 67 & 67 & 100 \\\\\n",
      "    \\textit{Ship} & 8 & 50 & 63 & 38 & 75 \\\\\n",
      "    \\textit{Snow} & 6 & 67 & 83 & 33 & 50 \\\\\n",
      "    \\textit{Tank} & 3 & 67 & 67 & 100 & 100 \\\\\n",
      "    \\textit{Traffic Light} & 5 & 40 & 40 & 20 & 20 \\\\\n",
      "    \\textit{Train} & 5 & 80 & 100 & 17 & 67 \\\\\n",
      "    \\textit{Truck} & 4 & 75 & 100 & 100 & 100 \\\\\n",
      "    \\textit{Wolf} & 6 & 17 & 50 & 86 & 86 \\\\\n",
      "    \\midrule\n",
      "    \\textbf{Average} & \\textbf{5.5} & \\textbf{60} & \\textbf{76} & \\textbf{64} & \\textbf{86} \\\\\n",
      "    \\bottomrule\n",
      "    \\end{tabular*}\n",
      "\\end{table}\n",
      "\n",
      "\\subsubsection*{Results for simple queries (Experiment~1)}\n",
      "In the first experiment we measured the top-$1$ and top-$5$ accuracy of our system with simple queries. \n",
      "The average accuracy for experiment 1 per game can be seen in Table~\\ref{tab:pergameresults1}, and per query in Table~\\ref{tab:results_t1}.\n",
      "The  overall average top-$1$ accuracy and average top-$5$ accuracy for `ViT-B/32' is 60\\% and 76\\% respectively, and for `RN101' we have 64\\% and 86\\% respectively.\n",
      "These results show that our system can identify a majority of objects in the game without fine-tuning or re-training.\n",
      "\n",
      "\\subsubsection*{Results for compound queries (Experiment~2)}\n",
      "In the second experiment we measure the top-$1$ and top-$5$ accuracy of our approach with compound queries.\n",
      "The average accuracy for experiment 2 per game can be seen in Table~\\ref{tab:pergameresults2}, and per query in Table~\\ref{tab:results_t2}.\n",
      "For the second experiment, we find that our approach shows particularly high performance for all of our selected games, except for The Witcher 3: Wild Hunt.\n",
      "Our approach achieves an overall average top-$5$ accuracy of \\textbf{78\\%} using `ViT-B/32' and \\textbf{82\\%} using the `RN101' model.\n",
      "These results show that our approach is flexible enough to effectively search gameplay videos with compound queries. \n",
      "\n",
      "\\begin{table}[t!]\n",
      "    \\caption{Average top-$k$ accuracy (\\%) per query for compound queries (Experiment~2). $N$ is the number of games searched.}\n",
      "    \\label{tab:results_t2}\n",
      "    \\centering\n",
      "    \\begin{tabular*}{\\linewidth}{l @{\\extracolsep{\\fill}} r r @{\\extracolsep{\\fill}} r r @{\\extracolsep{\\fill}} r}\n",
      "    \\toprule\n",
      "    \\textbf{} & \\textbf{} & \\multicolumn{2}{l}{\\textbf{ViT-B/32}} & \\multicolumn{2}{l}{\\textbf{RN101}} \\\\\n",
      "    \\textbf{Query} & \\textbf{$N$} & \\textbf{Top-$1$} & \\textbf{Top-$5$} & \\textbf{Top-$1$} & \\textbf{Top-$5$} \\\\\n",
      "    \\midrule\n",
      "    \\textit{A bald person} & 8 & 75 & 88 & 88 & 88 \\\\\n",
      "    \\textit{A bike on a mountain} & 4 & 25 & 75 & 50 & 75 \\\\\n",
      "    \\textit{A black car} & 5 & 80 & 100 & 80 & 100 \\\\\n",
      "    \\textit{A blue airplane} & 4 & 25 & 75 & 50 & 75 \\\\\n",
      "    \\textit{A blue car} & 5 & 80 & 80 & 40 & 100 \\\\\n",
      "    \\textit{A brown cow} & 7 & 29 & 71 & 57 & 71 \\\\\n",
      "    \\textit{A brown horse} & 3 & 100 & 100 & 100 & 100 \\\\\n",
      "    \\textit{A car on a mountain} & 4 & 75 & 75 & 75 & 100 \\\\\n",
      "    \\textit{A golden dragon} & 2 & 0 & 50 & 0 & 50 \\\\\n",
      "    \\textit{A gray tank} & 3 & 33 & 67 & 33 & 33 \\\\\n",
      "    \\textit{A man on top of a tank} & 4 & 25 & 50 & 0 & 0 \\\\\n",
      "    \\textit{A person in a jungle} & 7 & 57 & 100 & 57 & 100 \\\\\n",
      "    \\textit{A person on a mountain} & 7 & 71 & 100 & 57 & 100 \\\\\n",
      "    \\textit{A person wearing gold} & 8 & 50 & 88 & 50 & 100 \\\\\n",
      "    \\textit{A person wearing purple} & 8 & 50 & 88 & 25 & 63 \\\\\n",
      "    \\textit{A person with a pig mask} & 1 & 100 & 100 & 100 & 100 \\\\\n",
      "    \\textit{A police car} & 3 & 33 & 67 & 67 & 100 \\\\\n",
      "    \\textit{A police officer} & 3 & 33 & 33 & 67 & 100 \\\\\n",
      "    \\textit{A red car} & 5 & 80 & 100 & 80 & 100 \\\\\n",
      "    \\textit{A white airplane} & 4 & 75 & 75 & 50 & 100 \\\\\n",
      "    \\textit{A white horse} & 3 & 33 & 67 & 33 & 67 \\\\\n",
      "    \\textit{A white truck} & 5 & 40 & 60 & 60 & 80 \\\\\n",
      "    \\midrule\n",
      "    \\textbf{Average} & \\textbf{4.7} & \\textbf{53} & \\textbf{78} & \\textbf{55} & \\textbf{82} \\\\\n",
      "    \\bottomrule\n",
      "    \\end{tabular*}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table*}[t!]\n",
      "     \\caption{Recall @5 (\\%) for bug queries (Experiment~3). Queries that were not used per game are shown with values of `-'.}% Section~\\ref{subsec:formulation}).}\n",
      "     \\label{tab:results_t3}\n",
      "     \\centering\n",
      "\\begin{tabular*}{\\textwidth}{lr@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}rr@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r@{\\extracolsep{\\fill}}r}\n",
      "    \\toprule\n",
      "     & \\multicolumn{8}{l}{\\textbf{ViT-B/32}} & \\multicolumn{8}{l}{\\textbf{RN101}} \\\\\n",
      "     \\textbf{Query}  & \\smaller\\textbf{\\texttt{GTA}}  & \\smaller\\textbf{\\texttt{RDR}} & \\smaller\\textbf{\\texttt{JC3}}  & \\smaller\\textbf{\\texttt{F4}} & \\smaller\\textbf{\\texttt{ESV}} & \\smaller\\textbf{\\texttt{W3}}& \\smaller\\textbf{\\texttt{C77}} & \\smaller\\textbf{\\texttt{FC5}} & \\smaller\\textbf{\\texttt{GTA}} & \\smaller\\textbf{\\texttt{RDR}} & \\smaller\\textbf{\\texttt{JC3}} & \\smaller\\textbf{\\texttt{F4}} & \\smaller\\textbf{\\texttt{ESV}}  & \\smaller\\textbf{\\texttt{W3}}& \\smaller\\textbf{\\texttt{C77}} & \\smaller\\textbf{\\texttt{FC5}} \\\\\n",
      "    \\midrule\n",
      "\\smaller \\textit{A bike inside a car}                & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A bike on a wall}                   & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A car flying in the air}            & \\smaller 100 & \\smaller - & \\smaller 100 & \\smaller 40  & \\smaller - & \\smaller - & \\smaller 60  & \\smaller 80  & \\smaller 100 & \\smaller - & \\smaller 100 & \\smaller 40  & \\smaller - & \\smaller - & \\smaller 100 & \\smaller 80  \\\\\n",
      "\\smaller \\textit{A car on fire}                      & \\smaller 60  & \\smaller - & \\smaller 80  & \\smaller - & \\smaller - & \\smaller - & \\smaller 60  & \\smaller 80  & \\smaller 60  & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller 80  & \\smaller 100 \\\\\n",
      "\\smaller \\textit{A car in vertical position}         & \\smaller 100 & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller 80  & \\smaller 100 & \\smaller 100 & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller 80  & \\smaller 60  \\\\\n",
      "\\smaller \\textit{A car stuck in a rock}              & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - \\\\\n",
      "\\smaller \\textit{A car stuck in a tree}              & \\smaller 60  & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 60  & \\smaller 100 & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  \\\\\n",
      "\\smaller \\textit{A car under ground}                 & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - \\\\\n",
      "\\smaller \\textit{A carriage running over a person}   & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A dragon inside the floor}          & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A head without a body}              & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 0   & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A headless person}                  & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A helicopter inside a car}          & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  \\\\\n",
      "\\smaller \\textit{A horse floating the air}           & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A horse in the air}                 & \\smaller - & \\smaller 80  & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A horse in the fire}                & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A horse on fire}                    & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A horse on top of a building}       & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A horse to stand on its legs}       & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person falling inside the ground} & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person flying in the air}         & \\smaller 80  & \\smaller 100 & \\smaller 100 & \\smaller 60  & \\smaller 40  & \\smaller 100 & \\smaller 80  & \\smaller 100 & \\smaller 100 & \\smaller 100 & \\smaller 80  & \\smaller 100 & \\smaller 60  & \\smaller 100 & \\smaller 80  & \\smaller 100 \\\\\n",
      "\\smaller \\textit{A person goes through the ground}   & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 0   & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person in fire}                   & \\smaller - & \\smaller 100 & \\smaller - & \\smaller 60  & \\smaller 60  & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - & \\smaller 80  & \\smaller 60  & \\smaller 80  & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person inside a chair}            & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person inside a rock}             & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 80  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person on the house wall}         & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person stuck in a barrel}         & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person stuck in a tree}           & \\smaller 80  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller 80  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person stuck inside a wall}       & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person stuck on the }ceiling      & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person under a  vehicle}          & \\smaller 80  & \\smaller - & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller 0   & \\smaller - \\\\\n",
      "\\smaller \\textit{A person under a car}               & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person under a vehicle}           & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 80  \\\\\n",
      "\\smaller \\textit{A person under the carriage}        & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A person without head}              & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 20  & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A ship under water}                 & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 80  & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A tank in the air}                  & \\smaller 80  & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 80  & \\smaller - & \\smaller 80  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller \\textit{A vehicle inside the water}         & \\smaller 80  & \\smaller - & \\smaller 80  & \\smaller 40  & \\smaller - & \\smaller - & \\smaller 80  & \\smaller 100 & \\smaller 80  & \\smaller - & \\smaller 100 & \\smaller 40  & \\smaller - & \\smaller - & \\smaller 40  & \\smaller 80  \\\\\n",
      "\\smaller \\textit{A vehicle on top of building}       & \\smaller 100 & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - & \\smaller 100 & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - \\\\\n",
      "\\smaller \\textit{A vehicle on top of rooftop}        & \\smaller 60  & \\smaller - & \\smaller 80  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - & \\smaller 80  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - \\\\\n",
      "\\smaller  \\textit{An airplane in a tree}              & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 80  \\\\\n",
      "\\smaller  \\textit{An airplane in the water}           & \\smaller 20  & \\smaller - & \\smaller 60 & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 60  & \\smaller 40  & \\smaller - & \\smaller 80 & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 80  \\\\\n",
      "\\smaller  \\textit{Cars in accident}                   & \\smaller 100 & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller 100 & \\smaller 80  & \\smaller - & \\smaller 80  & \\smaller - & \\smaller - & \\smaller - & \\smaller 100 & \\smaller 100 \\\\\n",
      "\\smaller  \\textit{Two cars on top of each other}       & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 60  & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller - & \\smaller 40  & \\smaller -   \\\\          \n",
      "    \\midrule\n",
      "    \\smaller\\textbf{Average} & \\smaller\\textbf{77}\t& \\smaller\\textbf{60}\t& \\smaller\\textbf{82}\t& \\smaller\\textbf{44}\t& \\smaller\\textbf{44}\t& \\smaller\\textbf{67}\t& \\smaller\\textbf{67}\t& \\smaller\\textbf{78}\t& \\smaller\\textbf{83}\t& \\smaller\\textbf{50}\t& \\smaller\\textbf{87}\t& \\smaller\\textbf{46}\t& \\smaller\\textbf{51}\t& \\smaller\\textbf{67}\t& \\smaller\\textbf{60}\t& \\smaller\\textbf{76} \\\\\n",
      "    \\bottomrule\n",
      "\\end{tabular*}\n",
      "\\end{table*}\n",
      "\n",
      "\\subsubsection*{Results for bug queries (Experiment~3)}\n",
      "In the final experiment, we measure recall~@5 of our approach with bug queries. \n",
      "Table~\\ref{tab:results_t3} shows the results for Experiment~3 for each query with each game.\n",
      "Our approach shows particularly high performance for Grand Theft Auto~V, Just Cause~3, and Far Cry~5.\n",
      "% These results suggest that our approach is able to rank videos based on their relevance to the bug query in most cases. \n",
      "% Two video games, \\textit{Fallout 4} and \\textit{Skyrim} are more challenging for our system; the main reason is their artistic design and unusual elements. \n",
      "The average accuracy for Experiment~3 across all 44 unique queries is 66.12\\% and 66.35\\% using `ViT-B/32' and `RN101' respectively. \n",
      "% \\textit{is this a problem with approach? or with experiment? e.g. would a dev of the game run into the same issues?}\n",
      "These numbers suggest that, in most cases, our approach can reliably retrieve relevant videos based on an English text query containing a description of an event. \n",
      "Moreover, we can conclude that contrastive pre-training methods are powerful enough to be used in the video game domain, especially for bug detection in gameplay videos. \n",
      "\n",
      "\\section{Discussion} \\label{sec:discussion}\n",
      "\n",
      "\\begin{figure*}[!t]\n",
      "\t\\centering\n",
      "\t\\begin{subfigure}[t]{\\columnwidth}\n",
      "\t\t\\centering\n",
      "\t\t\\includegraphics[width=\\textwidth]{images/pairs/kl5d3z.jpeg}\n",
      "% \t\t\\captionsetup{justification=centering}\n",
      "\t\t\\caption{\\reddit{kl5d3z}{Video} of \\textit{`A head without a body'} from Fallout~4.}\n",
      "\t\t\\Description{Fully described in the text.}\n",
      "\t\t\\label{fig:sample_pair_1}\n",
      "\t\\end{subfigure}\n",
      "\t\\hfill\n",
      "\t\\begin{subfigure}[t]{\\columnwidth}\n",
      "\t\t\\centering\n",
      "\t\t\\includegraphics[width=\\textwidth]{images/pairs/g5pm35.jpeg}\n",
      "% \t\t\\captionsetup{justification=centering}\n",
      "\t\t\\caption{\\reddit{g5pm35}{Video} of \\textit{`A person stuck in a barrel'} from The Elder Scrolls~V: Skyrim.}\n",
      "\t\t\\Description{Fully described in the text.}\n",
      "\t\t\\label{fig:sample_pair_2}\n",
      "\t\\end{subfigure}\n",
      "\t\\hfill\n",
      "\t\\begin{subfigure}[t]{\\columnwidth}\n",
      "\t\t\\centering\n",
      "\t\t\\includegraphics[width=\\textwidth]{images/pairs/6xplqg.jpeg}\n",
      "% \t\t\\captionsetup{justification=centering}\n",
      "\t\t\\caption{\\reddit{6xplqg}{Video} of \\textit{`A car in a vertical position'} from Grand Theft Auto~V.}\n",
      "\t\t\\Description{Fully described in the text.}\n",
      "\t\t\\label{fig:sample_pair_3}\n",
      "\t\\end{subfigure}\n",
      "\t\\hfill\n",
      "\t\\begin{subfigure}[t]{\\columnwidth}\n",
      "\t\t\\centering\n",
      "\t\t\\includegraphics[width=\\textwidth]{images/pairs/8t7qfa.jpeg}\n",
      "% \t\t\\captionsetup{justification=centering}\n",
      "\t\t\\caption{\\reddit{8t7qfa}{Video} of \\textit{`A person stuck in a horse'} from The Witcher~3: Wild Hunt.}\n",
      "\t\t\\Description{Fully described in the text.}\n",
      "\t\t\\label{fig:sample_pair_4}\n",
      "\t\\end{subfigure}\n",
      "\t\\caption{Relevant gameplay videos identified using our approach with bug queries.}\n",
      "\t\\Description{Fully described in the text.}\n",
      "\t\\label{fig:more_example_pairs}\n",
      "\\end{figure*}\n",
      "\n",
      "%For this assessment, we subjectively evaluated the videos retrieved by our system. \n",
      "%Our first aim is to analyze specific queries to understand why retrieved results were wrong, or why the system was confused about its answer. \n",
      "%We also provide more detail on how certain queries relate to specific types of game physics bugs.\n",
      "In this section, we discuss the strengths and weaknesses of our approach, based on the results of our experiments.\n",
      "Figure~\\ref{fig:more_example_pairs} shows several example video frames from videos identified when searching gameplay videos with text queries using our approach.\n",
      "These examples help to illustrate the promising potential of our approach. \n",
      "% \\subsection{Promising zero-shot object identification performance}\n",
      "Given that our method does not require any training on gameplay videos, our zero-shot object and event identification results are promising.\n",
      "% In essence we can reduce the number of videos a developer has to review from ~2000 to, for example, just 5 when searching for a specific object or event.\n",
      "During our experiments, the first author manually analyzed each video returned by our search approach, including false positives.\n",
      "Below, the causes of false positives in our search results are detailed.\n",
      "\n",
      "\\subsubsection*{Adversarial poses}\n",
      "One important category of problems is the unusual pose of familiar objects. As extensively tested and reported by Alcorn et al. \\cite{alcorn2019strike}, neural networks occasionally misclassify objects when they have different poses than what they used to have in the training set. For example, consider a neural network that can detect a `car' in an image. It is possible to find a particular camera angle for which the neural network can not detect the `car' in that image. In a dataset of natural images, we might have lots of cars, but the camera angle and the position of the car relative to the camera do not vary a lot. A neural network trained on these datasets will struggle to detect a car when it sees it from a very unusual angle (e.g., when it is positioned vertically)\n",
      "\n",
      "\\subsubsection*{Confusing textures and patterns in the images}\n",
      "The textures and patterns can pose influential distractions and confusion for the neural network model in our approach. \n",
      "Sometimes a part of a game environment has a texture similar to another object. \n",
      "For example, our model confuses a striped colored wall in the Grand Theft Auto~V game with a `parachute.'  \n",
      "This category of problems is hard to encounter globally because each game has a diverse look and feel and creative artistic directions.\n",
      "% \\hl{this means we do not see it in the CLIP training set (often), right?}\n",
      "\n",
      "% \\subsection{Bug identification performance}\n",
      "% In the third task, we evaluated the performance of our approach in identifying gameplay videos that contain bugs (or events related to bugs).\n",
      "% Similar to the object identification tasks, we find that our approach has promising performance, but still face a significant number of false positives.\n",
      "% Some false positives are videos that are heavily related to the input text query without containing a bug.\n",
      "% We found that most false positives were specific to a particular game and text query.\n",
      "% For example, we noticed that when we are searching for terms related to `chair' and `tables' in the \\textit{Fallout 4} game, the system will retrieve videos with ragdoll issues. \n",
      "% % In the retrieved videos, when a character has a wrong posture, usually there is a chair nearby.\n",
      "% However, we did discover some common causes of gameplay videos being incorrectly identified when searching with bug queries, and we detail those causes below.\n",
      "\n",
      "% For \\textit{Cyberpunk 2077}, we noticed that extreme light reflections in a frame would cause the models to perform poorly. \n",
      "% Considering that the design and styles of objects in \\textit{Cyberpunk 2077} are very different from normal life, the results are still very promising.\n",
      "% Since \\textit{The Witcher 3: Wild Hunt} and \\textit{Skyrim} are similar in terms of magical elements, we compare their results. \n",
      "% Our system achieves a higher average recall rate on \\textit{The Witcher 3} compared to the \\textit{Skyrim} game. \n",
      "% We did a quick analysis and found that a large percentage of bugs in this game are related to the incorrect position or animation of a horse. \n",
      "% The fire and magic confusion is still present in this game but less severe.\n",
      "\n",
      "% \\subsubsection*{Finding clipping bugs using the `A falling man' query} \n",
      "% \\hl{not clear why it is important to highlight this?}\n",
      "% We found that the videos returned when searching for `A falling man` in the \\textit{Red Dead Redemption 2} game were all related to the ground clipping bug \\hl{need citation}. \n",
      "% In this type of bug, the player will escape the bounds of the world and will go through the ground. \n",
      "% All top retrieved samples for this query are of this type of bug. \n",
      "% We also tried this query against other video games. \n",
      "% Due to the difference in the distribution of the bugs in other games and the game mechanic, we only caught a few of these bugs in other video games. \n",
      "% % \\hl{not relevant to subsubsection*:\n",
      "% % Furthermore, we noticed that searching for query `a person flying in the air'  in the \\textit{Far Cry 5} game will yield a similar result, but the number of retrieved samples was lower.}\n",
      "\n",
      "\\subsubsection*{Confusion about different types of vehicles}\n",
      "During analysis of the videos that contain bugs related to cars, we noticed that sometimes some of the results partially match the textual description, except we see a bike instead of a car. \n",
      "Through our manual evaluation of the search results, we found that the model in our approach sometimes confused cars, bikes, airplanes, tanks, and other military vehicles. \n",
      "An instance of this misclassification is when we search for `A car on fire'. \n",
      "In some of the retrieved videos, we saw an airplane on fire instead of a car. \n",
      "\n",
      "\\subsubsection*{Confusion about different four-legged animals}\n",
      "After reviewing several results for queries related to animals, we found out that the model in our approach struggles to distinguish different animals. \n",
      "More specifically, for gameplay videos the CLIP model will confuse `dogs', `cats', `deer', `wolves', and sometimes `cows' and 'horses' with each other. \n",
      "A possible remedy for this problem is getting help from a separate pre-trained animal detection model to verify the CLIP model's prediction. \n",
      "% It is quite easy to train an animal detector neural network and use it to assist the search process.\n",
      "\n",
      "% \\subsubsection*{Finding ragdoll issues in \\textit{Fallout 4}}\n",
      "% We noticed that when we are searching for terms related to `chair' and `tables' in the \\textit{Fallout 4} game, the system will retrieve videos with ragdoll issues. \n",
      "% Especially when we searched for the query `a man sitting in a chair', all of the retrieved samples had a character with incorrect body position in them. \n",
      "% We further analyzed the retrieved videos to better understand the decision made by our system. \n",
      "% We found a spurious correlation between a chair object and incorrect posture in our dataset related to this video game. \n",
      "% In the retrieved videos, when a character has a wrong posture, usually there is a chair nearby. \n",
      "% More interestingly, when such bugs happen, the player usually stares at the bug for several seconds. \n",
      "% This causes several correct matches between the word `chair' and the video in our system, causing a higher similarity score.\n",
      "\n",
      "% \\subsubsection*{Fire and magic confusion in \\textit{Skyrim}}\n",
      "% After reviewing results for some queries related to the word `Fire', we found that our system has difficulty in distinguishing particle effects related to magic attacks in the \\textit{Skyrim} game.\n",
      "% Although these magical effects have different colors than fire, the similarity between textures and patterns will cause our model to confuse them. \n",
      "% \\hl{the question is: would a skyrim dev use those queries, also: isn't the query still true? just not a bug?}\n",
      "\n",
      "% \\subsubsection*{Snow vs. black and white effect}\n",
      "% Some videos contain black and white frames. \n",
      "% The videos are not in black and white, and this is just an in-game effect for specific events, e.g. when a player dies. \n",
      "% When we want to retrieve bug samples related to the `Snow' or `Winter', our system can not distinguish the frames in black and white from the frames containing snow in them.\n",
      "\n",
      "% \\subsubsection{Confusing `underwater' and `sky'}\n",
      "% Our proposed method sometimes confuses the sky and water because of the dominance of the blue color.\n",
      "% This issue happens in many video games, and the system incorrectly shows objects in the sky when we are using a bug query to search for an object in water or underwater. \n",
      "% For example, when we want to search for videos of a car inside water, our proposed method will also show videos of a vehicle in the sky.\n",
      "\n",
      "% \\subsubsection{Adversarial text patches}\n",
      "% We were surprised to see that the CLIP model can read some texts in the images (as well as identify objects and events).\n",
      "% This capability of the model will also cause some unexpected behaviors. \n",
      "% If a piece of text in the frame exactly matches the text query, this will cause a very high similarity score. \n",
      "% Depending on the use case of our system, this can be seen as a bug or feature.\n",
      "\n",
      "% \\subsubsection{Relational queries}\n",
      "% We performed extensive analysis to see if our neural network is powerful enough to answer relational queries. \n",
      "% For instance, we would like to find two objects on video that have a specific relationship, e.g. a person standing on top of a car. \n",
      "% While we had some success in retrieving videos that satisfy these types of relations, our system often can not find suitable videos. \n",
      "% A general example for failure cases is that the retrieved videos contain different objects in the query, but the specified relationship does not hold. \n",
      "% This limitation sometimes leads to some false-positive results.\n",
      "\n",
      "% \\subsubsection{Explaining the decision of the network}\n",
      "\n",
      "% WE WILL DO THIS ON THE FOLLOW UP PAPER\n",
      "\n",
      "% In this section, we used one of the popular explainable artificial intelligence (XAI) tools to shed more light on the system's results. \n",
      "% These tools help us to visualize the model's output and the regions in the input that contributes the most to the network's prediction. \n",
      "% One common visualization technique is to show a heatmap on top of the input image. \n",
      "% The dark red regions will represent pixels of the image that are more important for the network to make its prediction. \n",
      "% These kinds of model debugging and explainability tools will help us to understand if the model is looking at the correct regions of the input frame. For this task, We chose GradCAM \\cite{selvaraju2017grad, clipgradcam}.\n",
      "% As can be seen in Figure \\ref{fig:xai}, our system does correctly focus on the most relevant regions of the frames.\n",
      "% The highest importance is given to both the person and the walls in the frame, which is clearly desired for the query `A person stuck inside a wall'.\n",
      "\n",
      "% \\begin{figure}[!ht]\n",
      "% \\centering\n",
      "% \\includegraphics[width=8cm]{images/door-sample1.png}\n",
      "% \\includegraphics[width=8cm]{images/door-sample2.png}\n",
      "% \\caption{Explaining neural network's decision for the query `A person stuck inside a wall'.}\n",
      "% \\label{fig:xai}\n",
      "% \\end{figure}\n",
      "\n",
      "% \\hl{not needed - remove candidate}\n",
      "% \\hl{TBD}\n",
      "\n",
      "\\section{Limitations} \\label{sec:limitations}\n",
      "% In this section, we discuss the limitations of our current system and identify directions for improvement.\n",
      "\n",
      "\\subsection{Adversarial samples} \\label{subsec:adversarial}\n",
      "Every machine learning method suffers from a group of adversarial attacks and out-of-distribution samples. As described extensively in previous work~\\cite{alcorn2019strike}, any data point outside the training distribution is problematic for machine learning algorithms.\n",
      "Similarly, we observe some cases in which the neural network model makes an incorrect prediction. In particular, our model has some difficulty making a correct guess if it saw an object in an unfamiliar or adversarial pose. Due to physical simulations in video games, these adversarial poses are prevalent.\n",
      "\n",
      "Another observation we had is about text patches inside the games. The CLIP model has the ability to `read' the text inside an image as well. This feature is not something that the model was explicitly trained for, but rather some emergent behavior of pre-training in a contrastive setting. Sometimes searching a particular text query will result in retrieving the video that ignores the meaning of the text query, but the image contains that text. For example, if any video frames include a text field containing `a blue car', searching for the query `a blue car', will retrieve that video. Obviously, depending on the use case, this can be treated as both a feature and bug.\n",
      "\n",
      "% \\subsection{Using temporal information}\n",
      "% In this work, we performed the search process on individual frames of a video. Despite the promising results, this method cannot detect certain actions because it has no memory of what happened before. As an example, let's say we would like to find all the flashing light bulbs in the dataset. The current method can detect a light bulb, or even it can detect if it is on or off, but because it has no idea what happened in other frames of the video, it can not decide if the light is flashing or not. Of course, this is an elementary example, but detecting complex events in a video requires knowledge about the past. To overcome these issues, we need to consider temporal information in the search process. A possible solution for this is using other variants of the CLIP model that works with videos \\cite{wang2021actionclip, xu2021videoclip}.\n",
      "% \\textbf{Incorrect animation being played}:\n",
      "\n",
      "\\subsection{Improvements on search speed} \\label{subsec:searchspeed}\n",
      "In our proposed method, we calculate the embeddings of all frames in advance. With this pre-processing step, our system answers an arbitrary text query in just a few seconds. It might not be possible to perform this step in advance for some exceptional use cases. For handling such cases, there are some performance improvement techniques to run each neural network faster in inference mode, at the cost of sacrificing the model's accuracy. For example, it is possible to reduce the floating point precision of a model \\cite{courbariaux2014training} or even binarize the entire model \\cite{hubara2016binarized}. One simple but effective way to achieve faster runtime is to cut the last layers of the neural network gradually to reach an optimal performance vs. accuracy trade-off \\cite{9474052}. \n",
      "Using these techniques, or similar speed-up approaches, improving the presented system is possible.\n",
      "\n",
      "% \\subsection{Fine-tuning CLIP model on a game dataset}\n",
      "% We did not perform any fine-tuning on the original CLIP model. One of the future directions is to tailor the model for a video game domain. Despite the good generalization capability of the CLIP model, it was originally trained on natural images. We can create a specialized dataset with pairs of images and texts for video games. \n",
      "% Such a specialized dataset would bootstrap the development of new bug detection models.\n",
      "% While this task might be tedious, it is also possible to use some special transfer learning techniques to adapt existing datasets to new domains \\cite{kadish2021improving}.\n",
      "% \\subsection{Query expansion to improve accuracy}\n",
      "% \\hl{not needed - remove candidate}\n",
      "% Expand the query to improve accuracy!\n",
      "% \\textbf{Queries containing a name for a color}:\n",
      "% \\subsection{Broader Impact}\n",
      "\\section{Threats to validity} \\label{sec:threats}\n",
      "\n",
      "\\subsubsection*{Threats to internal validity.}\n",
      "Due to a lack of a benchmark dataset, we designed a set of custom queries for searching the gameplay videos in our \\texttt{GamePhysics} dataset.\n",
      "To address potential bias when generating these queries, the first author performed a pilot analysis of 80 gameplay videos across the 8 selected games to determine relevant objects and events before we designed the queries.\n",
      "\n",
      "In each of our experiments, we assumed that an accuracy measurement of 0\\% indicated that our approach failed to correctly identify any relevant videos.\n",
      "For example, in Experiment~3 we assumed that a recall~@5 of $0\\%$ in our search results indicated that our approach failed to identify that bug query in that game.\n",
      "However, it could instead be the case that our dataset does not contain any videos that match the query.\n",
      "Without a benchmark dataset, we do not have the ground truth for whether a repository of gameplay videos contains any matches for a given arbitrary text query.\n",
      "This means that the reported performance values are possibly lower estimates of the actual performance.\n",
      "% \\hl{query expansion is the other possible approach; instead of a person in a car, a man/woman/child in a car}\n",
      "\n",
      "In Experiment~3, we used our second aggregation method (Section~\\ref{subsec:aggregating}), which involved the selection of a pool size hyperparameter. \n",
      "Although we selected the default value of 1,000 based on manual trial and error, different selections of this hyperparameter could lead to different results for Experiment~3.\n",
      "Therefore, future research is required to understand how the selection of the pool size in our second aggregation method impacts the performance of our approach.\n",
      "\n",
      "\\subsubsection*{Threats to external validity.}\n",
      "While our dataset predominantly consists of gameplay videos that contain game physics bugs, our approach may not be as effective with other datasets of gameplay videos.\n",
      "Non-curated datasets may contain many more false positives (non-buggy gameplay), for example if using gameplay streaming footage.\n",
      "Additionally, we excluded long ($>$60 seconds) videos, meaning our approach may not be effective for long videos. We also ignored all videos with scores of zero from the GamePhysics subreddit. After manually checking a random sample of low-scored posts we observed that a score of 0 almost always indicated low quality/spam/etc.  This threshold might not be generalizable to other subreddits. \n",
      "Future research is required to evaluate the performance of our approach with long videos and non-curated datasets.\n",
      "\n",
      "\\section{Conclusion} \\label{sec:conclusion}\n",
      "In this paper, we proposed a novel approach to mine large repositories of gameplay videos by leveraging the zero-shot transfer capabilities of CLIP to connect video frames with an English text query.\n",
      "Our approach is capable of finding objects in a large dataset of videos, using simple and compound queries. \n",
      "Additionally, our approach shows promising performance in finding specific (bug-related) events, indicating it has the potential to be applied in automatic bug identification for video games.\n",
      "Even though we did not perform any fine-tuning or re-training to adapt the CLIP model to the video game domain, our approach performs surprisingly well on the majority of video games. \n",
      "We evaluated our system on a dataset of \\textbf{6,192} videos from eight games with different visual styles and elements. \n",
      "When experimenting with the bug queries, we measured recall~@5 and found the average accuracy of our approach across all 44 unique bug queries is \\textbf{66.24\\%} when averaged across both of the CLIP architectures utilized in our experiments.\n",
      "Furthermore, our manual analysis of the search results enabled us to discuss causes of false-positives in our approach and identify several future research directions. \n",
      "Our approach lays the foundation to utilizing contrastive learning models for zero-shot bug identification in video games.\n",
      "Future work in this line of research will provide more insights into video games bugs, and will pave the way to creating a new paradigm of automated bug detection methods for video games.\n",
      "\n",
      "\n",
      "    %\\printbibheading\n",
      "\t%\\AtNextBibliography{\\footnotesize}\n",
      "\t%\\newpage\n",
      "\t%\\clearpage \n",
      "% \t\\DeclareFieldFormat{titlecase}{#1}\n",
      " \t\n",
      " \t\\AtNextBibliography{\\footnotesize}%adjust size to normal ACM, because we use biblatex \n",
      "%  \t\\renewbibmacro{finentry}{%\n",
      "% \t\t\\iffieldequalstr{entrykey}{radford2021learning}%<- key after which you want the break\n",
      "% \t\t{\\finentry\\newpage}\n",
      "% \t{\\finentry}}\n",
      "\t\\printbibliography\n",
      "\\end{document}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(latex_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the anthroptic client and the ContextualQA model\n",
    "client = anthropic.Client(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "\n",
    "\n",
    "def load_context(paper_id):\n",
    "    latex_source = download_arxiv_source(paper_id)\n",
    "    model = ContextualQA(client, model=\"claude-v1.3-100k\")\n",
    "    model.load_text(latex_source)\n",
    "    return (\n",
    "        model,\n",
    "        [(f\"Load the paper with id {paper_id}.\", \"Paper loaded, Now ask a question.\")],\n",
    "    )\n",
    "\n",
    "\n",
    "def answer_fn(model, question, chat_history):\n",
    "\n",
    "    # if question is empty, tell user that they need to ask a question\n",
    "    if question == \"\":\n",
    "        chat_history.append((\"No Question Asked\", \"Please ask a question.\"))\n",
    "        return model, chat_history, \"\"\n",
    "\n",
    "    response = model.ask_question(question)\n",
    "\n",
    "    chat_history.append((question, response[0]['completion']))\n",
    "    return model, chat_history, \"\"\n",
    "\n",
    "def clear_context():\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/gradio/components.py:4650: UserWarning: The 'color_map' parameter has been deprecated.\n",
      "  warnings.warn(\"The 'color_map' parameter has been deprecated.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/gradio/components.py:163: UserWarning: Unknown style parameter: color_map\n",
      "  warnings.warn(f\"Unknown style parameter: {key}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# Explore ArXiv Papers in Depth with `claude-v1.3-100k` - Ask Questions and Receive Detailed Answers Instantly\")\n",
    "    gr.Markdown(\n",
    "        \"Dive into the world of academic papers with our dynamic app, powered by the cutting-edge `claude-v1.3-100k` model. This app allows you to ask detailed questions about any ArXiv paper and receive direct answers from the paper's content. Utilizing a context length of 100k tokens, it provides an efficient and comprehensive exploration of complex research studies, making knowledge acquisition simpler and more interactive. (This text is generated by GPT-4 )\"\n",
    "    )\n",
    "\n",
    "    with gr.Column():\n",
    "        with gr.Row():\n",
    "            paper_id_input = gr.Textbox(label=\"Enter Paper ID\", value=\"2303.10130\")\n",
    "            btn_load = gr.Button(\"Load Paper\")\n",
    "            qa_model = gr.State()\n",
    "\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot().style(color_map=(\"blue\", \"yellow\"))\n",
    "            question_txt = gr.Textbox(\n",
    "                label=\"Question\", lines=1, placeholder=\"Type your question here...\"\n",
    "            )\n",
    "            btn_answer = gr.Button(\"Answer Question\")\n",
    "\n",
    "            btn_clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    btn_load.click(load_context, inputs=[paper_id_input], outputs=[qa_model, chatbot])\n",
    "\n",
    "    btn_answer.click(\n",
    "        answer_fn,\n",
    "        inputs=[qa_model, question_txt, chatbot],\n",
    "        outputs=[qa_model, chatbot, question_txt],\n",
    "    )\n",
    "\n",
    "    question_txt.submit(\n",
    "        answer_fn,\n",
    "        inputs=[qa_model, question_txt, chatbot],\n",
    "        outputs=[qa_model, chatbot, question_txt],\n",
    "    )\n",
    "\n",
    "    btn_clear.click(clear_context, outputs=[chatbot])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: On the Opportunities and Risks of Foundation Models\n",
      "Abstract: AI is undergoing a paradigm shift with the rise of models (e.g., BERT,\n",
      "DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a\n",
      "wide range of downstream tasks. We call these models foundation models to\n",
      "underscore their critically central yet incomplete character. This report\n",
      "provides a thorough account of the opportunities and risks of foundation\n",
      "models, ranging from their capabilities (e.g., language, vision, robotics,\n",
      "reasoning, human interaction) and technical principles(e.g., model\n",
      "architectures, training procedures, data, systems, security, evaluation,\n",
      "theory) to their applications (e.g., law, healthcare, education) and societal\n",
      "impact (e.g., inequity, misuse, economic and environmental impact, legal and\n",
      "ethical considerations). Though foundation models are based on standard deep\n",
      "learning and transfer learning, their scale results in new emergent\n",
      "capabilities,and their effectiveness across so many tasks incentivizes\n",
      "homogenization. Homogenization provides powerful leverage but demands caution,\n",
      "as the defects of the foundation model are inherited by all the adapted models\n",
      "downstream. Despite the impending widespread deployment of foundation models,\n",
      "we currently lack a clear understanding of how they work, when they fail, and\n",
      "what they are even capable of due to their emergent properties. To tackle these\n",
      "questions, we believe much of the critical research on foundation models will\n",
      "require deep interdisciplinary collaboration commensurate with their\n",
      "fundamentally sociotechnical nature.\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "# The arXiv ID of the paper you're interested in\n",
    "paper_id = \"2108.07258\"\n",
    "\n",
    "# Create a search query with the arXiv ID\n",
    "search = arxiv.Search(id_list=[paper_id])\n",
    "\n",
    "# Fetch the paper using its arXiv ID\n",
    "paper = next(search.results())\n",
    "\n",
    "# Print the paper's title\n",
    "print(\"Title:\", paper.title)\n",
    "\n",
    "# Print the paper's abstract\n",
    "print(\"Abstract:\", paper.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
