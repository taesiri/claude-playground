{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with Arxiv using `claude-v1.3-100k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import openai\n",
    "import time\n",
    "import tempfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import gradio as gr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper - Get Arxiv Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_arxiv_source(paper_id):\n",
    "    url = f\"https://arxiv.org/e-print/{paper_id}\"\n",
    "\n",
    "    # Get the tar file\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Open the tar file\n",
    "    tar = tarfile.open(fileobj=io.BytesIO(response.content), mode=\"r\")\n",
    "\n",
    "    # Load all .tex files into memory, including their subdirectories\n",
    "    tex_files = {\n",
    "        member.name: tar.extractfile(member).read().decode(\"utf-8\")\n",
    "        for member in tar.getmembers()\n",
    "        if member.name.endswith(\".tex\")\n",
    "    }\n",
    "\n",
    "    # Pattern to match \\input{filename} and \\include{filename}\n",
    "    pattern = re.compile(r\"\\\\(input|include){(.*?)}\")\n",
    "\n",
    "    # Function to replace \\input{filename} and \\include{filename} with file contents\n",
    "    def replace_includes(text):\n",
    "        output = []\n",
    "        for line in text.split(\"\\n\"):\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                command, filename = match.groups()\n",
    "                # LaTeX automatically adds .tex extension for \\include command\n",
    "                if command == \"include\":\n",
    "                    filename += \".tex\"\n",
    "                if filename in tex_files:\n",
    "                    output.append(replace_includes(tex_files[filename]))\n",
    "                else:\n",
    "                    output.append(f\"% {line} % FILE NOT FOUND\")\n",
    "            else:\n",
    "                output.append(line)\n",
    "        return \"\\n\".join(output)\n",
    "\n",
    "    if \"main.tex\" in tex_files:\n",
    "        # Start with the contents of main.tex\n",
    "        main_tex = replace_includes(tex_files[\"main.tex\"])\n",
    "    else:\n",
    "        # No main.tex, concatenate all .tex files\n",
    "        main_tex = \"\\n\".join(replace_includes(text) for text in tex_files.values())\n",
    "\n",
    "    return main_tex\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Client(api_key=os.environ['ANTHROPIC_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualQA:\n",
    "    def __init__(self, client, model=\"claude-v1.3-100k\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.context = \"\"\n",
    "        self.questions = []\n",
    "        self.responses = []\n",
    "\n",
    "    def load_text(self, text):\n",
    "        self.context = text\n",
    "\n",
    "    def ask_question(self, question):\n",
    "        leading_prompt = \"Consider the text document below:\"\n",
    "        trailing_prompt = (\n",
    "            \"Now answer the following question, use Markdown to format your answer.\"\n",
    "        )\n",
    "        prompt = f\"{anthropic.HUMAN_PROMPT} {leading_prompt}\\n\\n{self.context}\\n\\n{trailing_prompt}\\n\\n{anthropic.HUMAN_PROMPT} {question} {anthropic.AI_PROMPT}\"\n",
    "        response = self.client.completion_stream(\n",
    "            prompt=prompt,\n",
    "            stop_sequences=[anthropic.HUMAN_PROMPT],\n",
    "            max_tokens_to_sample=6000,\n",
    "            model=self.model,\n",
    "            stream=False,\n",
    "        )\n",
    "        responses = [data for data in response]\n",
    "        self.questions.append(question)\n",
    "        self.responses.append(responses)\n",
    "        return responses\n",
    "\n",
    "    def clear_context(self):\n",
    "        self.context = \"\"\n",
    "        self.questions = []\n",
    "        self.responses = []\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        del state[\"client\"]\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        self.__dict__.update(state)\n",
    "        self.client = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_to_pdf(question, answer, output_filename):\n",
    "    latex_template = r\"\"\"\n",
    "    \\documentclass{{standalone}}\n",
    "    \\usepackage[utf8]{{inputenc}}\n",
    "    \\usepackage{{amsmath}}\n",
    "    \\usepackage{{amssymb}}\n",
    "    \\usepackage{{hyperref}}\n",
    "    \\usepackage{{varwidth}}\n",
    "    \\usepackage{{adjustbox}}\n",
    "    \\begin{{document}}\n",
    "    \\begin{{adjustbox}}{{margin=5mm}}\n",
    "    \\begin{{varwidth}}{{\\linewidth}}\n",
    "    \\textbf{{Question:}} \\\\\n",
    "    {question} \\\\\n",
    "    \\textbf{{Answer:}} \\\\\n",
    "    {answer}\n",
    "    \\end{{varwidth}}\n",
    "    \\end{{adjustbox}}\n",
    "    \\end{{document}}\n",
    "    \"\"\"\n",
    "\n",
    "    answer = answer[0][\"completion\"]\n",
    "    latex_content = latex_template.format(question=question, answer=answer)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        tex_file = Path(temp_dir) / \"qa.tex\"\n",
    "        pdf_file = Path(temp_dir) / \"qa.pdf\"\n",
    "\n",
    "        with open(tex_file, \"w\") as f:\n",
    "            f.write(latex_content)\n",
    "\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"pdflatex\",\n",
    "                \"-interaction=nonstopmode\",\n",
    "                \"-output-directory\",\n",
    "                temp_dir,\n",
    "                tex_file,\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "\n",
    "        shutil.copy(pdf_file, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411862"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_source = download_arxiv_source(\"2303.12712\")\n",
    "len(latex_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the anthroptic client and the ContextualQA model\n",
    "client = anthropic.Client(api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "\n",
    "\n",
    "def load_context(paper_id):\n",
    "    latex_source = download_arxiv_source(paper_id)\n",
    "    model = ContextualQA(client, model=\"claude-v1.3-100k\")\n",
    "    model.load_text(latex_source)\n",
    "    return (\n",
    "        model,\n",
    "        [(f\"Load the paper with id {paper_id}.\", \"Paper loaded, Now ask a question.\")],\n",
    "    )\n",
    "\n",
    "\n",
    "def answer_fn(model, question, chat_history):\n",
    "\n",
    "    # if question is empty, tell user that they need to ask a question\n",
    "    if question == \"\":\n",
    "        chat_history.append((\"No Question Asked\", \"Please ask a question.\"))\n",
    "        return model, chat_history, \"\"\n",
    "\n",
    "    response = model.ask_question(question)\n",
    "\n",
    "    chat_history.append((question, response[0]['completion']))\n",
    "    return model, chat_history, \"\"\n",
    "\n",
    "def clear_context():\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/gradio/components.py:4650: UserWarning: The 'color_map' parameter has been deprecated.\n",
      "  warnings.warn(\"The 'color_map' parameter has been deprecated.\")\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch2/lib/python3.10/site-packages/gradio/components.py:163: UserWarning: Unknown style parameter: color_map\n",
      "  warnings.warn(f\"Unknown style parameter: {key}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"### ContextualQA Chatbot\")\n",
    "\n",
    "    with gr.Column():\n",
    "        with gr.Row():\n",
    "            paper_id_input = gr.Textbox(label=\"Enter Paper ID\", value=\"2303.10130\")\n",
    "            btn_load = gr.Button(\"Load Paper\")\n",
    "            qa_model = gr.State()\n",
    "\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot().style(color_map=(\"blue\", \"yellow\"))\n",
    "            question_txt = gr.Textbox(\n",
    "                label=\"Question\", lines=1, placeholder=\"Type your question here...\"\n",
    "            )\n",
    "            btn_answer = gr.Button(\"Answer Question\")\n",
    "\n",
    "            btn_clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    btn_load.click(load_context, inputs=[paper_id_input], outputs=[qa_model, chatbot])\n",
    "\n",
    "    btn_answer.click(\n",
    "        answer_fn,\n",
    "        inputs=[qa_model, question_txt, chatbot],\n",
    "        outputs=[qa_model, chatbot, question_txt],\n",
    "    )\n",
    "\n",
    "    question_txt.submit(\n",
    "        answer_fn,\n",
    "        inputs=[qa_model, question_txt, chatbot],\n",
    "        outputs=[qa_model, chatbot, question_txt],\n",
    "    )\n",
    "\n",
    "    btn_clear.click(clear_context, outputs=[chatbot])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
